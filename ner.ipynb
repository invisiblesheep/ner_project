{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'tags': ['I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn']]\n",
      "[['I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-PER', 'I-PER']]\n"
     ]
    }
   ],
   "source": [
    "# Load our training data\n",
    "import json\n",
    "import random\n",
    "import numpy\n",
    "with open(\"data/ner_train.json\") as f:\n",
    "    data=json.load(f)\n",
    "print(data[0])\n",
    "\n",
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"tags\"] for one_example in data] # This is now a list of lists just like the texts variable\n",
    "print(texts[:2])\n",
    "print(labels[:2])\n",
    "\n",
    "# Lets do the same thing for the validation data\n",
    "# We use a separate validation set, since generally using sentences from the same documents as train/validation results in overly optimistic scores\n",
    "with open(\"data/ner_test.json\") as f:\n",
    "    validation_data=json.load(f)\n",
    "validation_texts=[one_example[\"text\"] for one_example in validation_data]\n",
    "validation_labels=[one_example[\"tags\"] for one_example in validation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n",
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Use gensim to read the embedding model\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model=KeyedVectors.load_word2vec_format(\"data/wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words=[k for k,v in sorted(vector_model.vocab.items(), key=lambda x:x[1].index)]\n",
    "print(\"Words from embedding model:\",len(words))\n",
    "print(\"First 50 words:\",words[:50])\n",
    "\n",
    "# Normalize the vectors\n",
    "\n",
    "print(\"Before normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\",vector_model.get_vector(\"in\")[:10])\n",
    "\n",
    "# Build vocabulary mappings\n",
    "\n",
    "vocabulary={\"<SPECIAL>\": 0, \"<OOV>\": 1} # zero has a special meaning in sequence models, prevent using it for a normal word\n",
    "for word in words:\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inversed_vocabulary={value:key for key, value in vocabulary.items()} # inverse the dictionary\n",
    "\n",
    "# Label mappings\n",
    "label_set = set([label for sentence_labels in labels for label in sentence_labels])\n",
    "label_map = {label: index for index, label in enumerate(label_set)}\n",
    "                \n",
    "# Embedding matrix\n",
    "\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings=numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def vectorizer(vocab, texts, label_map, labels=None):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    vectorized_labels = [] # same thing for the labels\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(texts):\n",
    "        vectorized_example = []\n",
    "        vectorized_example_labels = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "        if labels:\n",
    "            for label in labels[i]:\n",
    "                vectorized_example_labels.append(label_map[label])\n",
    "\n",
    "        vectorized_data.append(vectorized_example)\n",
    "        vectorized_labels.append(vectorized_example_labels)\n",
    "        \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy matrix\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    \n",
    "    return vectorized_data, vectorized_labels, sentence_lengths\n",
    "\n",
    "vectorized_data, vectorized_labels, lengths=vectorizer(vocabulary, texts, label_map, labels)\n",
    "validation_vectorized_data, validation_vectorized_labels, validation_lengths=vectorizer(vocabulary, validation_texts, label_map, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuomas/virtualenvs/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (14041,)\n",
      "New shape: (14041, 113)\n",
      "First example: [ 1587 11424   718   537     7 10975   379 14078     4     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "Padded labels shape: (14041, 113, 1)\n",
      "{'I-ORG': 0, 'I-PER': 1, 'I-LOC': 2, 'O': 3}\n",
      "First example labels: [[0]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "First weight vector: [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='post', maxlen=max(lengths))\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\", vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post', maxlen=max(lengths)), -1)\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "print(label_map)\n",
    "print(\"First example labels:\", vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\", weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post', maxlen=max(lengths))\n",
    "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post', maxlen=max(lengths)), -1)\n",
    "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
    "validation_weights[validation_weights > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def evaluate(predictions, gold, lengths):\n",
    "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
    "    \n",
    "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
    "    \n",
    "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
    "    pred_count = sum([len(e) for e in pred_entities])\n",
    "    try:\n",
    "        precision = tp / pred_count\n",
    "        recall = tp / sum([len(e) for e in gold_entities])\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    except Exception as e:\n",
    "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
    "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
    "\n",
    "\n",
    "def _convert_to_entities(input_sequence):\n",
    "    \"\"\"\n",
    "    Reads a sequence of tags and converts them into a set of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    previous_tag = label_map['O']\n",
    "    for i, tag in enumerate(input_sequence):\n",
    "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "            current_entity.append((tag, i))\n",
    "        elif tag == label_map['O']: # Entity has ended\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "        elif tag == previous_tag: # Current entity continues\n",
    "            current_entity.append((tag, i))\n",
    "        previous_tag = tag\n",
    "    \n",
    "    # Add the last entity to our entity list if the sentences ends with an entity\n",
    "    if len(current_entity) > 0:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    entity_offsets = set()\n",
    "    \n",
    "    for e in entities:\n",
    "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
    "    \n",
    "    return entity_offsets\n",
    "\n",
    "class EvaluateEntities(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        evaluate(pred, validation_vectorized_labels_padded, validation_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, Conv1D, TimeDistributed, LSTM, Bidirectional, MaxPooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "\n",
    "vector_size= pretrained.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False)(inp)\n",
    "hidden = TimeDistributed(Dense(100, activation=\"softmax\"))(embeddings)\n",
    "outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "lr_dense=0.1\n",
    "batch_size_dense=100\n",
    "epochs_dense=100\n",
    "\n",
    "optimizer=Adam(lr=lr_dense) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist_dense=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=batch_size_dense,verbose=1,epochs=epochs_dense, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=model.predict(validation_vectorized_data_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1, Batch size: 100, Epochs: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8XXWd//HX5y7ZtzZJS9t0hYItS1soFQSUH4oCMuKw6bC4jMowOiPOqL8Rfy4//f1mdMb56YwLIggKyuACouggIwgIDLRQSinQFrrQ0nRLmjZbk5vc3Hx+f5yTkKbZmubmtjnv5+ORR+9y7rmfk5Pe9/1+v+d8j7k7IiIiALFcFyAiIkcOhYKIiPRSKIiISC+FgoiI9FIoiIhIL4WCiIj0UijIEcfMfmxm/3eEy24xs3cc7nrGmpkVmtlvzazJzH5pZleb2R/GaN2DbrPI4VIoiGTH5cBUoNLdr3D3u9z9neNdhJn9DzN7NAynLcMsu9DMVprZvvDnYTNb2G+ZU83scTNrNbPdZnZDn+cWm9kT4XvVmtkXs7RZkkUKBZHsmA286u5dOa5jP3A78NkRLLuDIMwmA1XA/cDPep40syrgQeAHQCVwHNC39fMfwOPh698GfNzM3nP4myDjSaEgoxJ2YXzWzNaY2X4zu83MpprZ782sJfyWOanP8u8xs5fNrNHMHjOzBX2eW2Jmq8LX/Rwo6PdeF5vZ6vC1T5nZKaOs+WNmttHM9prZ/WY2PXzczOxbZlZnZs1m9qKZnRQ+d5GZrQ1r225mnxnB+3wF+BLwvvAb9UfM7ENm9mSfZdzMrjezDeF2fc/MLHzuWDN7xMwazGyPmd1lZhWj2WZ3f8bdfwJsHsGyje6+xYNpDgzIEHzw9/h74L/CVk+Hu7e4+7o+z88B7nL3jLtvAp4EThxN3ZI7CgU5HJcB5wPHA38G/B74PFBN8Lf1SQAzOx64G/hU+NwDwG/NLM/M8oBfAz8h+Ib5y3C9hK9dQvBN968Ivp3+ALjfzPIPpVAzOw/4GnAlMA3Yyhvfgt8JvDXcjvJwmYbwuduAv3L3UuAk4JHh3svdvwz8E/Bzdy9x99sGWfRi4HTglPA939VTbljrdGABMBP434Ns19lm1jhcTYciXF8K+A7BdvQ4A9gbBnNdOGYyq8/z/wZ8wMySZnYCcCbw8FjWJtmnUJDD8R133+3u24EngBXu/ry7p4D7gCXhcu8D/tPdH3L3NPCvQCHwFoIPmiTwb+6edvd7gGf7vMd1wA/cfUX4DfQOoCN83aG4Grjd3Ve5ewdwI3Cmmc0B0kAp8CbA3H2du+8MX5cGFppZmbvvc/dVh/i+Q/l6+O38deBRYDGAu28Mf1cd7l4PfJOgO+Yg7v6ku4+qFTGYcH3lwN8Az/d5qgb4IHADMAt4jSDse/yOoPupHVgP3ObuffelHAUUCnI4dve53T7A/ZLw9nSCb+YAuHs3sA2YET633Q+cmXFrn9uzgU+HXSyN4bfYmeHrDkX/GloJWgMz3P0R4LvA94A6M7vFzMrCRS8DLgK2mtmfzOzMQ3zfoezqc7uN8PcVdsP9LOyuagZ+StDHP27cfT9wM3CnmU0JH24H7nP3Z8Pg/wrwFjMrN7PJBOMNXyXo/psJvMvMPj6edcvhUyjIeNhB8OEOBH34BB8a24GdwIye/vRQ3y6JbcA/untFn58id+/7DXU0NRQTdEdtB3D3b7v7acBCgm6kz4aPP+vulwBTCLq5fnGI7zsa/wQ4cLK7lwHXEHQpjbcYUEQQ3gBrwrp69L09D8i4+53u3uXutQTdcxeNS6UyZhQKMh5+AbzbzN5uZkng0wRdQE8BTwNdwCfDvuhLgWV9XnsrcL2ZvTkcEC42s3ebWekh1nA38OHwsMl8gg/eFe6+xcxOD9efJDhaJwV0h2MeV5tZedjt1Qx096wwHCw+dzS/kGGUAq1Ak5nNYGRHDg3IzGJmVkDQRWdmVhCO4wy07PnhoH88bCl9E9gH9Awm/wj48/B3mAS+CDzp7k3Aq+H6rwrf8xiCbsM1o61dckOhIFnn7q8QfNv9DrCHYFD6z9y90907gUuBDwF7CT5IftXntSuBjxF07+wDNobLHmoNDxN8iN1L0Do5Fnh/+HQZQfjsI+hiagC+ET53LbAl7Ma5nmBsAjObCbQALx5qLSPwFeBUoAn4T/r8Pvozs3PMrHWIdb2VoNvnAYIWWDt9DiO14Iiwq8O7FQTh2QRsIvgdXRB2FRF2s30+rKmO4Mikq8Lnmgn2498R/B5XAy8BOTl5UEbPdJEdkUNnZtcAJ7r7jbmuRWQsKRRERKSXuo9ERKSXQkFERHopFEREpFci1wUcqqqqKp8zZ06uyxAROao899xze9y9erjljrpQmDNnDitXrsx1GSIiRxUz2zr8Uuo+EhGRPhQKIiLSS6EgIiK9jroxhYGk02lqa2tJpVK5LiXrCgoKqKmpIZlM5roUEZmAJkQo1NbWUlpaypw5czhwss2Jxd1paGigtraWuXPn5rocEZmAJkT3USqVorKyckIHAoCZUVlZGYkWkYjkxoQIBWDCB0KPqGyniOTGhAmF4aTSGXY1pejKdA+/sIhIREUmFDrSGepaUqS7x35W2MbGRm666aZDft1FF11EY+OYXnNdROSwRCYUerpdsjFV+GCh0NXVNeTrHnjgASoqxvSa6yIih2VCHH00ErGwKz4bl4/43Oc+x6ZNm1i8eDHJZJKCggImTZrE+vXrefXVV3nve9/Ltm3bSKVS3HDDDVx33XXAG1N2tLa2cuGFF3L22Wfz1FNPMWPGDH7zm99QWFg49sWKiAxhwoXCV377Mmt3NB/0eLc77Z0ZCpJx4rFDG6xdOL2ML//ZiYM+//Wvf52XXnqJ1atX89hjj/Hud7+bl156qfew0dtvv53JkyfT3t7O6aefzmWXXUZlZeUB69iwYQN33303t956K1deeSX33nsv11xzzSHVKSJyuCZcKBwJli1bdsB5BN/+9re57777ANi2bRsbNmw4KBTmzp3L4sWLATjttNPYsmXLuNUrItJjwoXCYN/oU+kMr+5uYdbkIiqK8rJaQ3Fxce/txx57jIcffpinn36aoqIizj333AHPM8jPz++9HY/HaW9vz2qNIiIDidBAc/BvNsYUSktLaWlpGfC5pqYmJk2aRFFREevXr2f58uVjX4CIyBiZcC2FwcQIUqE7C6lQWVnJWWedxUknnURhYSFTp07tfe6CCy7g5ptvZsGCBZxwwgmcccYZY/7+IiJjxbJxiGY2LV261PtfZGfdunUsWLBgyNd1ZbpZu7OZ6RWFVJXkD7nskW4k2ysi0peZPefuS4dbLkLdR9lrKYiITBSRCYVsnqcgIjJRTJhQGK4bzMwwLCtnNI+no71+ETmyTYhQKCgooKGhYQTBAFmY+mjc9FxPoaCgINeliMgENSGOPqqpqaG2tpb6+vohl6trbKclL05Tls9TyKaeK6+JiGTDhAiFZDI5oiuRfeRrf+Ss46r4xhU6ckdEZCATovtopPKTcTq6dD0FEZHBRCsUEjE6ujK5LkNE5IiVtVAwswIze8bMXjCzl83sKwMs8yEzqzez1eHPR7NVD/SEgloKIiKDyeaYQgdwnru3mlkSeNLMfu/u/Sf/+bm7/00W6+iVn4iTSqulICIymKy1FDzQGt5Nhj85PSA0P6mWgojIULI6pmBmcTNbDdQBD7n7igEWu8zM1pjZPWY2c5D1XGdmK81s5XCHnQ4lPxGjI61QEBEZTFZDwd0z7r4YqAGWmdlJ/Rb5LTDH3U8BHgLuGGQ9t7j7UndfWl1dPep68hNxDTSLiAxhXI4+cvdG4FHggn6PN7h7R3j3h8Bp2axDA80iIkPL5tFH1WZWEd4uBM4H1vdbZlqfu+8B1mWrHtCYgojIcLJ59NE04A4zixOEzy/c/Xdm9lVgpbvfD3zSzN4DdAF7gQ9lsZ6g+0hHH4mIDCproeDua4AlAzz+pT63bwRuzFYN/amlICIytIid0RxMc6Hpp0VEBhaxUAg2tzOj1oKIyEAiGQrqQhIRGVi0QiEZB9BUFyIig4hWKPS0FHRWs4jIgKIZCuo+EhEZUMRCIeg+0lQXIiIDi1YoJNVSEBEZSrRCQWMKIiJDilgoqPtIRGQoEQsFdR+JiAwlUqFQkOxpKSgUREQGEqlQeGNMQd1HIiIDiVYo6OgjEZEhRSsUEuo+EhEZSsRCIdhczX0kIjKwSIaCWgoiIgOLVCiYGXmJmM5TEBEZRKRCAYLWgs5oFhEZWARDIa7uIxGRQUQwFNR9JCIymOiFQjKmloKIyCAiFwoFibjGFEREBhG5UAhaCuo+EhEZSPRCIaHuIxGRwUQwFHT0kYjIYCIYCjHNkioiMojohUJSLQURkcFELxTUUhARGVQ0Q0EtBRGRAUUwFNR9JCIymOiFgs5TEBEZVNZCwcwKzOwZM3vBzF42s68MsEy+mf3czDaa2Qozm5OtenrkJ2KkM06m27P9ViIiR51sthQ6gPPcfRGwGLjAzM7ot8xHgH3ufhzwLeCfs1gP8MYlOTvVhSQicpCshYIHWsO7yfCn/9fzS4A7wtv3AG83M8tWTQAFyZ6rr6kLSUSkv6yOKZhZ3MxWA3XAQ+6+ot8iM4BtAO7eBTQBlQOs5zozW2lmK+vr6w+rpp6WggabRUQOltVQcPeMuy8GaoBlZnbSKNdzi7svdfel1dXVh1VT73WaNVOqiMhBxuXoI3dvBB4FLuj31HZgJoCZJYByoCGbteSH3UcpdR+JiBwkm0cfVZtZRXi7EDgfWN9vsfuBD4a3LwcecfesHhbU232kloKIyEESWVz3NOAOM4sThM8v3P13ZvZVYKW73w/cBvzEzDYCe4H3Z7EeoE/3kVoKIiIHyVoouPsaYMkAj3+pz+0UcEW2ahjIG6GgloKISH8RPKO55+gjtRRERPqLXijo6CMRkUFFNxTUfSQicpDohYK6j0REBhW5UChQS0FEZFCRC4XeloLGFEREDhK9UNB5CiIig4pcKCRiRswgpZaCiMhBIhcKZhZeklMtBRGR/iIXCtBzSU61FERE+otmKCRiGmgWERlARENB3UciIgOJaCio+0hEZCDRDAWNKYiIDCiaoaDuIxGRAUU0FDTQLCIykEiGQkEyru4jEZEBRDIUgoFmdR+JiPQX2VDQNBciIgeLaChooFlEZCDRDAUdkioiMqBohoKOPhIRGVBEQyHoPnL3XJciInJEiWgoxOh26OpWKIiI9BXNUEjqOs0iIgOJZigkeq7TrCOQRET6GlEomNkNZlZmgdvMbJWZvTPbxWXLG9dpVktBRKSvkbYU/tLdm4F3ApOAa4GvZ62qLCtIhi0FhYKIyAFGGgoW/nsR8BN3f7nPY0edN1oK6j4SEelrpKHwnJn9gSAU/svMSoGj9mt2z0CzproQETlQYoTLfQRYDGx29zYzmwx8OHtlZVdBONDc1tmV40pERI4sI20pnAm84u6NZnYN8AWgKXtlZVdlST4ADa2dOa5EROTIMtJQ+D7QZmaLgE8Dm4A7s1ZVlk0pDUJhd3Mqx5WIiBxZRhoKXR7MCXEJ8F13/x5QOtQLzGymmT1qZmvN7GUzu2GAZc41syYzWx3+fOnQN+HQVRQlyYvHqG/pGI+3ExE5aox0TKHFzG4kOBT1HDOLAclhXtMFfNrdV4UD08+Z2UPuvrbfck+4+8WHVvbhMTOqS/OpUyiIiBxgpC2F9wEdBOcr7AJqgG8M9QJ33+nuq8LbLcA6YMZh1DqmppTlU9ei7iMRkb5GFAphENwFlJvZxUDK3Uc8pmBmc4AlwIoBnj7TzF4ws9+b2YmDvP46M1tpZivr6+tH+rZDmlKaT12zWgoiIn2NdJqLK4FngCuAK4EVZnb5CF9bAtwLfCo8K7qvVcBsd18EfAf49UDrcPdb3H2puy+trq4eydsOa0ppgbqPRET6GemYwv8CTnf3OgAzqwYeBu4Z6kVmliQIhLvc/Vf9n+8bEu7+gJndZGZV7r5npBswWlPL8mlqT5NKZ3qnvRARibqRjinEegIh1DDca83MgNuAde7+zUGWOSZcDjNbFq6zYYQ1HZYppQUAOgJJRKSPkbYUHjSz/wLuDu+/D3hgmNecRXC00otmtjp87PPALAB3vxm4HPhrM+sC2oH3+zhdDq26LDhXoa4lxczJRePxliIiR7wRhYK7f9bMLiP4oAe4xd3vG+Y1TzLMpHnu/l3guyOpYaz1nMCmwWYRkTeMtKWAu99LMD4wIfR0H2mwWUTkDUOGgpm1AAN15xjg7l6WlarGQWVxHvGYaaoLEZE+hgwFdx9yKoujWSxmVJforGYRkb4ieY3mHsFZzQoFEZEe0Q6F0nzq1H0kItIr0qFQXVqg8xRERPqIdChMLcunYX8n6YwuyykiAhEPBZ3VLCJyoIiHQs9ZzQoFERGIeij0THWhwWYRESDqoaCzmkVEDhDpUKgqycNMoSAi0iPSoZCIx6gszqdel+UUEQEiHgoQDDbv1kypIiKAQiGc6kItBRERUCiEU12opSAiAgoFppYVsKe1g0z3uFzwTUTkiBb5UJhSmk+3Q8N+tRZERCIfCtU95yqoC0lERKEwNTyreVeTBptFRCIfCsdOKQFg/a7mHFciIpJ7kQ+FsoIk86qLWb2tKdeliIjkXORDAWBxTQWrtzXiriOQRCTaFArAopkV7GntYKfGFUQk4hQKwCk15QCsqW3McSUiIrmlUAAWTCsjGTeNK4hI5CkUgIJknAXTynhhm1oKIhJtCoXQKTXlvLi9iW5NdyEiEaZQCC2qqaC1o4vNe1pzXYqISM4oFEKLZ1YAaFxBRCJNoRCaV11CcV5c4woiEmkKhVA8ZpxcU67DUkUk0rIWCmY208weNbO1Zvaymd0wwDJmZt82s41mtsbMTs1WPSOxaGYFa3c209GVyWUZIiI5k82WQhfwaXdfCJwBfMLMFvZb5kJgfvhzHfD9LNYzrEU1FaQzzrqdLbksQ0QkZ7IWCu6+091XhbdbgHXAjH6LXQLc6YHlQIWZTctWTcNZMisYbF6+uSFXJYiI5NS4jCmY2RxgCbCi31MzgG197tdycHBgZteZ2UozW1lfX5+tMplWXsiimRX89oUdWXsPEZEjWdZDwcxKgHuBT7n7qC5a4O63uPtSd19aXV09tgX2c8mi6by8o5mNdepCEpHoyWoomFmSIBDucvdfDbDIdmBmn/s14WM5c/Ep04gZ3L9arQURiZ5sHn1kwG3AOnf/5iCL3Q98IDwK6Qygyd13ZqumkZhSVsCZx1Zy/ws7dH0FEYmcbLYUzgKuBc4zs9Xhz0Vmdr2ZXR8u8wCwGdgI3Ap8PIv1jNgli2awpaGNNbU6u1lEoiWRrRW7+5OADbOMA5/IVg2j9a6TjuELv36J36zewaJw+gsRkSjQGc0DKC9Mcu4J1fxuzQ4ymjVVRCJEoTCISxbPoK6lQ+csiEikKBQG8fYFUygrSPDT5VtzXYqIyLhRKAyiIBnn2jNn8+DLu3htz/5clyMiMi4UCkP40FvmkozHuOXxzbkuRURkXCgUhlBdms/lp9Vw76pa6lpSuS5HRCTrFArD+Ng580hnuvnxf2/JdSkiIlmnUBjG3KpiLjjxGH6yfCutHV25LkdEJKsUCiNw/duOpSXVxR1Pbcl1KSIiWaVQGIFFMys4f+FUvvPIBl5vaMt1OSIiWaNQGKGvXnIiiViM//XrFzVRnohMWAqFEZpWXsg/XHACT2zYw33P53R2bxGRrFEoHIKr3zyb02ZP4v/8bi0NrR25LkdEZMwpFA5BLGZ8/dKTae3o4m/vfp62Th2NJCITi0LhEM2fWso/X3YKyzc3cO1tz9DUns51SSIiY0ahMAqXnlrD9646lTW1jVx163J1JYnIhKFQGKULT57GrR9Yysa6Vq66dQV793fmuiQRkcOmUDgM554whR996HS2NOznmh+uoLFNwSAiRzeFwmF6y3FV3BK2GD5w+zM0pzTGICJHL4XCGHjb8dXcdPWprN3RzMfuWEk6053rkkRERkWhMEbesXAq37jiFFa8tpd/eXB9rssRERkVhcIY+vMlNXzgzNnc+sRrPPDizlyXIyJyyBQKY+wL717IklkVfPaXL7CxrjXX5YiIHBKFwhjLS8S46epTKUjG+eDtz/D86/tyXZKIyIgpFLJgWnkhP/rw6QBccfPT3PTYRrq7NbOqiBz5FApZckpNBQ/ccA7vOvEY/uXBV7j29hXsbtZ1nkXkyKZQyKLywiTfvWoJX7/0ZFZtbeTCf3+Ch9fuznVZIiKDUihkmZnx/mWz+O3fns0xZQV89M6V/MM9a3j+9X26WI+IHHHsaPtgWrp0qa9cuTLXZYxKR1eGbzz4Cnc8vYV0xplRUcilp87gk2+fTzKufBaR7DGz59x96bDLKRTGX1N7mofW7uY/1+zg0VfquWTxdL555WLiMct1aSIyQY00FBLjUYwcqLwwyeWn1XD5aTXc9NhG/uXBVyhIxPnapScTUzCISA4pFHLs4+ceR3tnhu88spFYDC4/bSazJhdRVZKHmQJCRMZX1kLBzG4HLgbq3P2kAZ4/F/gN8Fr40K/c/avZqudI9vfnH08qneHWJ17j7me2AUFr4u/PP55rz5it1oOIjJusjSmY2VuBVuDOIULhM+5+8aGsdyKMKQxmc30rWxr2s21vOw+v280TG/awbM5kvnbZyaQz3aza2sgru5p587xKzl84VYPTIjJiOR9TcPfHzWxOttY/Ec2rLmFedQkAHzhzNveu2s5Xf/syb/9/f+pdJi8e446nt1JVks+VS2v4y7PnUlWSn6uSRWSCyfWYwplm9gKwg6DV8PJAC5nZdcB1ALNmzRrH8nLHzLj8tBreOr+Knz+7jRmTCjlt9iRqJhXx+Kv13LXidW7+0yZ+unwrn7twAe8/faa6mUTksGX1kNSwpfC7QbqPyoBud281s4uAf3f3+cOtcyJ3Hx2qjXWtfOHXL7J8816WzKrg3OOn9D53ysxy3jq/Woe5ighwhJynMFQoDLDsFmCpu+8ZajmFwoHcnfue384/PbCePa0dBzw3o6KQ950+k9mVRWzb20btvnaK8xMsmzuZZXMmM6k4L0dVi8h4y/mYwnDM7Bhgt7u7mS0jmHKjIVf1HK3MjEtPreHPl8ygJ9/T3d38cV0d/7Hidb750Ku9y1YW59HS0cVtTwYHfC2dPYmPnjOX8xceoxaFiADZPST1buBcoMrMaoEvA0kAd78ZuBz4azPrAtqB9/vRdnr1EcTM6DmtIT8W56KTp3HRydPY3thOW0cXNZOKKMyL09GV4YVtTSzf3MA9z9Vy/U9XMbeqmPcsmk5lSR6lBQkqivKYXl7ItIoCygqSuDtd3U57OkNTW5qm9jSZbufkGeUaxxCZYDTNRYRlup0HX9rFLU9s5oVtjQMuk4gZXYNcC2JuVTEfesscLj+thuL8BB1dGVLpbsoLk9ksW0RG4YgYU8gGhUJ2dHRlaEl10ZLqYu/+DnY0ptjZ1M6+tjTJeIy8uJGXiFFRmEd5UZKWVBc/Xb6V1dsayQvPl+jMdAMwa3IRbzu+mnPmV7FgWhnTKwoH7J7KdDu1+9qYVJxHWYGCRCSbFAoyLla9vo/fv7iTRDxGSX6CmBnPbtnL05saaE9nAEjGjZpJRVQUJSnJT1CUF2dnU4pXd7eQSneTjBvnzK/mwpOO4ZjyArbtbWfbvjaSMWPBtDIWTCujKD/OxrpWNta1sm9/mknFSSqKgu6uRMyIm1FSkGDhtDISIzipr7Wjiy179nPclBIKkvFs/5pEck6hIDnVM3YRnKXdxra9bTS1p2nt6GJ/RxdTyvI5YWoZx08tYVN9Kw+8uIvtje29r0/EjG53DvUqpqUFCd5ybCWn1FSwvbGdjXWtbN/XTmlBgklFeRSH4bKloQ2A4rw471g4lQtPOobCvAQNrR3s3d/JklmTOHVWRe/8U3UtKe58aistqTTHTS3luOoSygoTtKS6aE11UV6UZFFNBXkJnWUuo9eSSvPj/95CIh5jdmURcyqLedMxpWMydqdQkKOKu/PyjmZaO7qYObmIY8oKSGe6eXV3C2t3NNOezjB/Sinzp5YwuTiPpvY0+/Z30pzqotudTLdT39LBU5v28Pire9je2E55YZLjppQwc1Ih+zsz7NvfSWtHF3MqizlxehmzKotYvrmB37+0i8a29EE1nTi9jKvfPJv1u5r52bPb6Mp0U5iMs78zM+A2FOXFOWNeJSfNKCc/ESMvHqMoP8608gKmVxRSlEzwWsN+NtW10tSe5sxjK1k6e9KQLZuuTDfPbd3H05sbKC1IMq+qmFmVRbR3ZtjR2M7u5hSxmFFWkKSsMElhMk5e+N5VpXlUl+RndWLFuuYUT29uYE1tE3Oqijlz3mSOrS45oidz3NWUoqIoecS1EDfWtfJXP1nJpvr9Bzx+4vQyvnjxQs6YV3lY61coSGS5O60dXZTkJ0b04RTMK7WPeMyoLMmnOD/OH17ezU+e3soru1tIxo3LTq3h+rcdy+zKInY1p9iwu5W2zgxlBQlKChLsaEzx5MZ6ntywp7cVMhLlhUnePHcy3e40p7po78xQmIxTnB/v7YprTnWN+ndRWpDguCklzJ9SwvFTSzl+aikzJxdRmIxTmIzTlu7ilV0tvLKrhb37O5leUcjMyYWUFSTZ0tDGpvpWavcFLbi4gQOtqS6a2tPUt3awNdzWZNxIZ4LPkqqSfM4+rpK3nVDNW+dXUznG07C0d2bYunc/JfkJygqTlOQlhv0m3ZXp5g9rd/Pjp7bwzGt7yU/EWDpnEmfOqyQRj1HfErQQZ1cW8Y4FUzlxelnv304qnSFmltVW4IMv7eIzv3yB/ESM71y1hJNnlLO1oY01tU1895EN7GhK8a4Tp3LjhQuYU1U8qvdQKIgcJnfnxe1NVJfmM628cMSv6+520t3dpDNOa6qLHU3t7GhsZ3/YSplXXUJhXpwnXq3n4XV1PP/6PvKTcUoLgvGWVDpDa0cXHeluFs+s4Lw3TeHs+VV0dnXz2p79bG1oozg/zrTyQqaVF+BAc3twqHAq3U1nJkNnVze7mlJsrG9RIU3qAAAIcElEQVTtHYvZ09o5ZN158VjvwQI9knFjekUhMQu689yDoCkvTDKpKI9FM8s5Y14lJ04vp3ZfG8s3N/DUpgae3LCHhv3B+00qCpYtL0qSiAXhkc50057O0JoKuhM7urrx8Heel4gxqSiPiqI8qkvzmTW5kFmTi+h2eHLDHp7ZspfOru4D6p5bVcz8qSXMrSrGnd7172ntYHdzB1sb9rOntZOaSYW8b+lM9rWleWrTHtbvagGgMBlncnEeO5racQ9O/KwoSrKzKcXe/Z3EY8bsyUUcO6WEqpI8Mt1OphuK8+PMrSpmblUxMyoKyU8ELbXCvDhlBUN/KWls6+Q3q3fwy+e28dL2ZhbVlPP9a05jesWBf2updIYfPrGZmx7bxF8sm8UXL1444r/FvhQKInKAhtYOXtndwq6mFKl08KGZFzeOn1rKCceUUl6YpL61g21722lq72ROZTGzJheNaOC+v+7uoDvw8Q31vUexNbUF57ckEzGSMaMgL05pfoLi/AR5iRgxA8Po6Mqwry1NY1snu5s7eD0cjwI4fmoJb51fzSkzK0h1ZmhOpalv6WBjXSuv7G6hdl87ZkFQ5CdiVJXmM7W0gGnlBVx48jTOe9OUA46Ea2pPk4gZxfnBKVv1LR08ur6OP67fTWdXN9MqCpleXkAq3c2mMGAb29PEzYjHjOZUmpZBWnKJmIVH1gXrdoeMO+2dGdo7M+zv7KLbYeG0Mq5YWsNfLJs1ZJdWXXOK/GR81Id8KxREZMJoakvTmemmunTorqjubh/XEyrdnT2tnWyub2VXc4rOrm46M920d2bYu7+TfW2dNLd3gYEB8ZgFXXd5wYf7+QuncuL08nGp9Yif5kJEZKTKi0b27Xi8z7A3M6pL84cNq6OJjp8TEZFeCgUREemlUBARkV4KBRER6aVQEBGRXgoFERHppVAQEZFeCgUREel11J3RbGb1wNZRvrwK2DOG5RwtorjdUdxmiOZ2R3Gb4dC3e7a7Vw+30FEXCofDzFaO5DTviSaK2x3FbYZobncUtxmyt93qPhIRkV4KBRER6RW1ULgl1wXkSBS3O4rbDNHc7ihuM2RpuyM1piAiIkOLWktBRESGoFAQEZFekQkFM7vAzF4xs41m9rlc15MNZjbTzB41s7Vm9rKZ3RA+PtnMHjKzDeG/k3JdazaYWdzMnjez34X355rZinCf/9zM8nJd41gyswozu8fM1pvZOjM7Mwr72sz+Lvz7fsnM7jazgom4r83sdjOrM7OX+jw24P61wLfD7V9jZqeO9n0jEQpmFge+B1wILAT+wsxGd/XrI1sX8Gl3XwicAXwi3M7PAX909/nAH8P7E9ENwLo+9/8Z+Ja7HwfsAz6Sk6qy59+BB939TcAigm2f0PvazGYAnwSWuvtJQBx4PxNzX/8YuKDfY4Pt3wuB+eHPdcD3R/umkQgFYBmw0d03u3sn8DPgkhzXNObcfae7rwpvtxB8SMwg2NY7wsXuAN6bmwqzx8xqgHcDPwzvG3AecE+4yITabjMrB94K3Abg7p3u3kgE9jXBZYQLzSwBFAE7mYD72t0fB/b2e3iw/XsJcKcHlgMVZjZtNO8blVCYAWzrc782fGzCMrM5wBJgBTDV3XeGT+0CpuaorGz6N+B/At3h/Uqg0d27wvsTbZ/PBeqBH4VdZj80s2Im+L529+3AvwKvE4RBE/AcE3tf9zXY/h2zz7iohEKkmFkJcC/wKXdv7vucB8cgT6jjkM3sYqDO3Z/LdS3jKAGcCnzf3ZcA++nXVTRB9/Ukgm/Fc4HpQDEHd7FEQrb2b1RCYTsws8/9mvCxCcfMkgSBcJe7/yp8eHdPUzL8ty5X9WXJWcB7zGwLQdfgeQT97RVhFwNMvH1eC9S6+4rw/j0EITHR9/U7gNfcvd7d08CvCPb/RN7XfQ22f8fsMy4qofAsMD88QiGPYGDq/hzXNObCfvTbgHXu/s0+T90PfDC8/UHgN+NdWza5+43uXuPucwj27SPufjXwKHB5uNiE2m533wVsM7MTwofeDqxlgu9rgm6jM8ysKPx779nuCbuv+xls/94PfCA8CukMoKlPN9MhicwZzWZ2EUG/cxy43d3/MccljTkzOxt4AniRN/rWP08wrvALYBbBtONXunv/AawJwczOBT7j7heb2TyClsNk4HngGnfvyGV9Y8nMFhMMrOcBm4EPE3zRm9D72sy+AryP4Gi754GPEvSfT6h9bWZ3A+cSTJG9G/gy8GsG2L9hQH6XoCutDfiwu68c1ftGJRRERGR4Uek+EhGREVAoiIhIL4WCiIj0UiiIiEgvhYKIiPRSKIiMIzM7t2cWV5EjkUJBRER6KRREBmBm15jZM2a22sx+EF6rodXMvhXO5f9HM6sOl11sZsvDeezv6zPH/XFm9rCZvWBmq8zs2HD1JX2ug3BXeOKRyBFBoSDSj5ktIDhj9ix3XwxkgKsJJl9b6e4nAn8iOMMU4E7gH9z9FIKzyXsevwv4nrsvAt5CMKsnBLPXforg2h7zCObuETkiJIZfRCRy3g6cBjwbfokvJJh4rBv4ebjMT4Ffhdc1qHD3P4WP3wH80sxKgRnufh+Au6cAwvU94+614f3VwBzgyexvlsjwFAoiBzPgDne/8YAHzb7Yb7nRzhHTd06eDPp/KEcQdR+JHOyPwOVmNgV6r4s7m+D/S89MnFcBT7p7E7DPzM4JH78W+FN45btaM3tvuI58Mysa160QGQV9QxHpx93XmtkXgD+YWQxIA58guJDNsvC5OoJxBwimML45/NDvma0UgoD4gZl9NVzHFeO4GSKjollSRUbIzFrdvSTXdYhkk7qPRESkl1oKIiLSSy0FERHppVAQEZFeCgUREemlUBARkV4KBRER6fX/AbehjgIt8wskAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Learning rate: {}, Batch size: {}, Epochs: {}'.format(lr_dense, batch_size_dense, epochs_dense))\n",
    "plt.plot(hist_dense.history['loss'])\n",
    "#plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss, final: {}'.format(round(hist_dense.history['loss'][-1], 3)))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 113, 100)          160400    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,161,404\n",
      "Trainable params: 160,804\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "14041/14041 [==============================] - 1s 61us/step - loss: 0.4009\n",
      "\n",
      "Precision/Recall/F-score: 0.7028242939265184 / 0.5601593625498008 / 0.6234342090677308\n",
      "Epoch 2/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.1618\n",
      "\n",
      "Precision/Recall/F-score: 0.7371296490859425 / 0.698804780876494 / 0.7174557725738828\n",
      "Epoch 3/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.1293\n",
      "\n",
      "Precision/Recall/F-score: 0.7492795389048992 / 0.7250996015936255 / 0.736991293784167\n",
      "Epoch 4/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.1165\n",
      "\n",
      "Precision/Recall/F-score: 0.7598271249228237 / 0.7354581673306773 / 0.74744407328677\n",
      "Epoch 5/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.1093\n",
      "\n",
      "Precision/Recall/F-score: 0.7694068678459938 / 0.7364541832669322 / 0.7525699745547074\n",
      "Epoch 6/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.1049\n",
      "\n",
      "Precision/Recall/F-score: 0.7699792960662526 / 0.7408366533864542 / 0.7551269035532996\n",
      "Epoch 7/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.1004\n",
      "\n",
      "Precision/Recall/F-score: 0.77004477004477 / 0.7537848605577689 / 0.7618280652305215\n",
      "Epoch 8/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0970\n",
      "\n",
      "Precision/Recall/F-score: 0.7805283960890368 / 0.747410358565737 / 0.7636104609748651\n",
      "Epoch 9/100\n",
      "14041/14041 [==============================] - 1s 44us/step - loss: 0.0943\n",
      "\n",
      "Precision/Recall/F-score: 0.7718743827770097 / 0.7784860557768924 / 0.7751661211940891\n",
      "Epoch 10/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0914\n",
      "\n",
      "Precision/Recall/F-score: 0.7655918130913304 / 0.7898406374501992 / 0.7775272085498578\n",
      "Epoch 11/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0890\n",
      "\n",
      "Precision/Recall/F-score: 0.7699612403100775 / 0.7914342629482072 / 0.7805500982318272\n",
      "Epoch 12/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0867\n",
      "\n",
      "Precision/Recall/F-score: 0.7758176412289396 / 0.7796812749003984 / 0.7777446597118729\n",
      "Epoch 13/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0845\n",
      "\n",
      "Precision/Recall/F-score: 0.7790927021696252 / 0.7868525896414342 / 0.7829534192269574\n",
      "Epoch 14/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0824\n",
      "\n",
      "Precision/Recall/F-score: 0.7952820512820513 / 0.7723107569721116 / 0.7836280949974734\n",
      "Epoch 15/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0805\n",
      "\n",
      "Precision/Recall/F-score: 0.7754041570438799 / 0.8025896414342629 / 0.7887627251370399\n",
      "Epoch 16/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0789\n",
      "\n",
      "Precision/Recall/F-score: 0.799876593994241 / 0.7747011952191235 / 0.7870876340821696\n",
      "Epoch 17/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0769\n",
      "\n",
      "Precision/Recall/F-score: 0.7767736973658912 / 0.8047808764940239 / 0.7905293024165932\n",
      "Epoch 18/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0756\n",
      "\n",
      "Precision/Recall/F-score: 0.7920993687639992 / 0.7749003984063745 / 0.7834054979357566\n",
      "Epoch 19/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0740\n",
      "\n",
      "Precision/Recall/F-score: 0.8023617153511498 / 0.7715139442231076 / 0.7866355235096983\n",
      "Epoch 20/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0725\n",
      "\n",
      "Precision/Recall/F-score: 0.7858823529411765 / 0.798406374501992 / 0.792094861660079\n",
      "Epoch 21/100\n",
      "14041/14041 [==============================] - 1s 44us/step - loss: 0.0713\n",
      "\n",
      "Precision/Recall/F-score: 0.765466816647919 / 0.8133466135458167 / 0.7886807031099092\n",
      "Epoch 22/100\n",
      "14041/14041 [==============================] - 1s 48us/step - loss: 0.0700\n",
      "\n",
      "Precision/Recall/F-score: 0.7749000190439916 / 0.8105577689243028 / 0.792327913542985\n",
      "Epoch 23/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0688\n",
      "\n",
      "Precision/Recall/F-score: 0.7876106194690266 / 0.797808764940239 / 0.7926768926274121\n",
      "Epoch 24/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0676\n",
      "\n",
      "Precision/Recall/F-score: 0.7881589299763966 / 0.7982071713147411 / 0.793151227236738\n",
      "Epoch 25/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0666\n",
      "\n",
      "Precision/Recall/F-score: 0.7612926918839515 / 0.8258964143426295 / 0.7922797630422319\n",
      "Epoch 26/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0657\n",
      "\n",
      "Precision/Recall/F-score: 0.7973165277495425 / 0.7812749003984064 / 0.7892142066606298\n",
      "Epoch 27/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0647\n",
      "\n",
      "Precision/Recall/F-score: 0.8031784841075794 / 0.7852589641434263 / 0.7941176470588236\n",
      "Epoch 28/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0639\n",
      "\n",
      "Precision/Recall/F-score: 0.7881273188830307 / 0.80398406374502 / 0.7959767281333202\n",
      "Epoch 29/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0625\n",
      "\n",
      "Precision/Recall/F-score: 0.7867647058823529 / 0.8099601593625498 / 0.7981939536709854\n",
      "Epoch 30/100\n",
      "14041/14041 [==============================] - 1s 48us/step - loss: 0.0619\n",
      "\n",
      "Precision/Recall/F-score: 0.7871181163650516 / 0.8057768924302788 / 0.7963382222659711\n",
      "Epoch 31/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0610\n",
      "\n",
      "Precision/Recall/F-score: 0.7827506723011909 / 0.8117529880478087 / 0.7969880696264424\n",
      "Epoch 32/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0602\n",
      "\n",
      "Precision/Recall/F-score: 0.7991053273688491 / 0.7828685258964143 / 0.7909036023344737\n",
      "Epoch 33/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0592\n",
      "\n",
      "Precision/Recall/F-score: 0.757090909090909 / 0.8294820717131474 / 0.7916349809885931\n",
      "Epoch 34/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0589\n",
      "\n",
      "Precision/Recall/F-score: 0.7794590505012294 / 0.8209163346613546 / 0.7996507228097409\n",
      "Epoch 35/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0580\n",
      "\n",
      "Precision/Recall/F-score: 0.7879438158408115 / 0.8045816733067729 / 0.7961758328405283\n",
      "Epoch 36/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0571\n",
      "\n",
      "Precision/Recall/F-score: 0.7895351099007976 / 0.8085657370517928 / 0.7989371124889282\n",
      "Epoch 37/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0564\n",
      "\n",
      "Precision/Recall/F-score: 0.7915201250488472 / 0.8069721115537849 / 0.7991714342079306\n",
      "Epoch 38/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0558\n",
      "\n",
      "Precision/Recall/F-score: 0.8031861262351281 / 0.7934262948207171 / 0.7982763803988375\n",
      "Epoch 39/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0552\n",
      "\n",
      "Precision/Recall/F-score: 0.7862388867413993 / 0.8103585657370518 / 0.7981165391406709\n",
      "Epoch 40/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0546\n",
      "\n",
      "Precision/Recall/F-score: 0.7950445986124877 / 0.7990039840637451 / 0.7970193740685545\n",
      "Epoch 41/100\n",
      "14041/14041 [==============================] - 1s 44us/step - loss: 0.0540\n",
      "\n",
      "Precision/Recall/F-score: 0.7844427244582043 / 0.8075697211155378 / 0.795838241067923\n",
      "Epoch 42/100\n",
      "14041/14041 [==============================] - 1s 48us/step - loss: 0.0533\n",
      "\n",
      "Precision/Recall/F-score: 0.8019486975541857 / 0.8033864541832669 / 0.8026669320330381\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0526\n",
      "\n",
      "Precision/Recall/F-score: 0.7876685934489402 / 0.8143426294820717 / 0.8007835455435847\n",
      "Epoch 44/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0521\n",
      "\n",
      "Precision/Recall/F-score: 0.8033582844426461 / 0.7910358565737052 / 0.7971494529760113\n",
      "Epoch 45/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0518\n",
      "\n",
      "Precision/Recall/F-score: 0.8070564516129032 / 0.797410358565737 / 0.8022044088176353\n",
      "Epoch 46/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0511\n",
      "\n",
      "Precision/Recall/F-score: 0.8065619967793881 / 0.7982071713147411 / 0.802362835402483\n",
      "Epoch 47/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0510\n",
      "\n",
      "Precision/Recall/F-score: 0.8135523613963039 / 0.7892430278884462 / 0.8012133468149646\n",
      "Epoch 48/100\n",
      "14041/14041 [==============================] - 1s 44us/step - loss: 0.0504\n",
      "\n",
      "Precision/Recall/F-score: 0.809668284789644 / 0.797410358565737 / 0.8034925732637496\n",
      "Epoch 49/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0498\n",
      "\n",
      "Precision/Recall/F-score: 0.8049516908212561 / 0.796613545816733 / 0.8007609130957148\n",
      "Epoch 50/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0493\n",
      "\n",
      "Precision/Recall/F-score: 0.780751708428246 / 0.8193227091633466 / 0.7995723172628304\n",
      "Epoch 51/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0490\n",
      "\n",
      "Precision/Recall/F-score: 0.7829383886255924 / 0.8227091633466136 / 0.8023312287518213\n",
      "Epoch 52/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0486\n",
      "\n",
      "Precision/Recall/F-score: 0.7875766871165644 / 0.8183266932270916 / 0.8026572880031262\n",
      "Epoch 53/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0484\n",
      "\n",
      "Precision/Recall/F-score: 0.8113284433577832 / 0.7932270916334662 / 0.8021756647864624\n",
      "Epoch 54/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0476\n",
      "\n",
      "Precision/Recall/F-score: 0.7962781586679726 / 0.8097609561752988 / 0.8029629629629629\n",
      "Epoch 55/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0472\n",
      "\n",
      "Precision/Recall/F-score: 0.7854967643700038 / 0.8221115537848606 / 0.8033871909674907\n",
      "Epoch 56/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0469\n",
      "\n",
      "Precision/Recall/F-score: 0.795846394984326 / 0.8091633466135458 / 0.8024496246542868\n",
      "Epoch 57/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0464\n",
      "\n",
      "Precision/Recall/F-score: 0.7954634337113805 / 0.8103585657370518 / 0.802841918294849\n",
      "Epoch 58/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0463\n",
      "\n",
      "Precision/Recall/F-score: 0.8074509009921037 / 0.7944223107569721 / 0.8008836228537001\n",
      "Epoch 59/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0461\n",
      "\n",
      "Precision/Recall/F-score: 0.8066801619433198 / 0.7938247011952191 / 0.8002008032128515\n",
      "Epoch 60/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0454\n",
      "\n",
      "Precision/Recall/F-score: 0.7826666666666666 / 0.8185258964143426 / 0.8001947419668938\n",
      "Epoch 61/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0456\n",
      "\n",
      "Precision/Recall/F-score: 0.7958702064896755 / 0.8061752988047809 / 0.8009896091044038\n",
      "Epoch 62/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0451\n",
      "\n",
      "Precision/Recall/F-score: 0.7924016282225237 / 0.8143426294820717 / 0.8032223204636998\n",
      "Epoch 63/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0447\n",
      "\n",
      "Precision/Recall/F-score: 0.7934464599180807 / 0.8103585657370518 / 0.8018133438454715\n",
      "Epoch 64/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0447\n",
      "\n",
      "Precision/Recall/F-score: 0.7788061956932376 / 0.8213147410358566 / 0.7994958309094434\n",
      "Epoch 65/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0442\n",
      "\n",
      "Precision/Recall/F-score: 0.7801968951154865 / 0.8209163346613546 / 0.8000388274121529\n",
      "Epoch 66/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0440\n",
      "\n",
      "Precision/Recall/F-score: 0.7768297307546455 / 0.8161354581673307 / 0.7959976685447834\n",
      "Epoch 67/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0439\n",
      "\n",
      "Precision/Recall/F-score: 0.7804098514758413 / 0.8268924302788845 / 0.8029790115098172\n",
      "Epoch 68/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0434\n",
      "\n",
      "Precision/Recall/F-score: 0.7916502560063017 / 0.8007968127490039 / 0.7961972667855021\n",
      "Epoch 69/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0430\n",
      "\n",
      "Precision/Recall/F-score: 0.7898002714756641 / 0.8113545816733068 / 0.8004323474501326\n",
      "Epoch 70/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0431\n",
      "\n",
      "Precision/Recall/F-score: 0.786681930929212 / 0.8213147410358566 / 0.8036253776435046\n",
      "Epoch 71/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0429\n",
      "\n",
      "Precision/Recall/F-score: 0.7843398742617641 / 0.8201195219123506 / 0.8018307527509981\n",
      "Epoch 72/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0426\n",
      "\n",
      "Precision/Recall/F-score: 0.8007593924860112 / 0.7982071713147411 / 0.7994812450119713\n",
      "Epoch 73/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0422\n",
      "\n",
      "Precision/Recall/F-score: 0.8083383930378466 / 0.7956175298804781 / 0.8019275173175383\n",
      "Epoch 74/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0422\n",
      "\n",
      "Precision/Recall/F-score: 0.7938664596273292 / 0.8147410358565738 / 0.804168305151396\n",
      "Epoch 75/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0419\n",
      "\n",
      "Precision/Recall/F-score: 0.7846652679763494 / 0.8195219123505976 / 0.8017148981779207\n",
      "Epoch 76/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0420\n",
      "\n",
      "Precision/Recall/F-score: 0.7843062200956937 / 0.8163346613545817 / 0.8\n",
      "Epoch 77/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0417\n",
      "\n",
      "Precision/Recall/F-score: 0.7931167826759474 / 0.8171314741035857 / 0.804945054945055\n",
      "Epoch 78/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0414\n",
      "\n",
      "Precision/Recall/F-score: 0.8010752688172043 / 0.801394422310757 / 0.801234813782115\n",
      "Epoch 79/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0416\n",
      "\n",
      "Precision/Recall/F-score: 0.7908950617283951 / 0.8167330677290837 / 0.8036064288514309\n",
      "Epoch 80/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0413\n",
      "\n",
      "Precision/Recall/F-score: 0.7847395733230829 / 0.8133466135458167 / 0.7987870488115035\n",
      "Epoch 81/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0411\n",
      "\n",
      "Precision/Recall/F-score: 0.7788571428571428 / 0.8145418326693227 / 0.7962999026290165\n",
      "Epoch 82/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0411\n",
      "\n",
      "Precision/Recall/F-score: 0.782411775806756 / 0.8258964143426295 / 0.8035662370384726\n",
      "Epoch 83/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0407\n",
      "\n",
      "Precision/Recall/F-score: 0.7913178898189605 / 0.8097609561752988 / 0.800433198779167\n",
      "Epoch 84/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0404\n",
      "\n",
      "Precision/Recall/F-score: 0.7798550171690195 / 0.8143426294820717 / 0.7967257844474761\n",
      "Epoch 85/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0401\n",
      "\n",
      "Precision/Recall/F-score: 0.7922658375437233 / 0.8121513944223108 / 0.8020853826480424\n",
      "Epoch 86/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0404\n",
      "\n",
      "Precision/Recall/F-score: 0.7951524628616107 / 0.8103585657370518 / 0.802683504340963\n",
      "Epoch 87/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0403\n",
      "\n",
      "Precision/Recall/F-score: 0.7616770871643986 / 0.8250996015936255 / 0.792120864410021\n",
      "Epoch 88/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0402\n",
      "\n",
      "Precision/Recall/F-score: 0.7796803652968036 / 0.8163346613545817 / 0.7975866095757104\n",
      "Epoch 89/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0402\n",
      "\n",
      "Precision/Recall/F-score: 0.7958626073380172 / 0.8123505976095617 / 0.8040220820189273\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0398\n",
      "\n",
      "Precision/Recall/F-score: 0.7871116225546605 / 0.8175298804780876 / 0.80203244088333\n",
      "Epoch 91/100\n",
      "14041/14041 [==============================] - 1s 44us/step - loss: 0.0396\n",
      "\n",
      "Precision/Recall/F-score: 0.783957623912221 / 0.8254980079681274 / 0.8041917329710847\n",
      "Epoch 92/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0399\n",
      "\n",
      "Precision/Recall/F-score: 0.7945505171042164 / 0.795816733067729 / 0.7951831210191083\n",
      "Epoch 93/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0395\n",
      "\n",
      "Precision/Recall/F-score: 0.7907425265188043 / 0.8167330677290837 / 0.8035276825085742\n",
      "Epoch 94/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0393\n",
      "\n",
      "Precision/Recall/F-score: 0.7855495772482706 / 0.8143426294820717 / 0.7996870109546167\n",
      "Epoch 95/100\n",
      "14041/14041 [==============================] - 1s 46us/step - loss: 0.0394\n",
      "\n",
      "Precision/Recall/F-score: 0.7935333073626801 / 0.8115537848605577 / 0.8024423872365569\n",
      "Epoch 96/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0393\n",
      "\n",
      "Precision/Recall/F-score: 0.7874685747437633 / 0.8111553784860558 / 0.7991364929840055\n",
      "Epoch 97/100\n",
      "14041/14041 [==============================] - 1s 45us/step - loss: 0.0394\n",
      "\n",
      "Precision/Recall/F-score: 0.7805155420773313 / 0.8203187250996016 / 0.7999222999223\n",
      "Epoch 98/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0388\n",
      "\n",
      "Precision/Recall/F-score: 0.7795290543106722 / 0.8177290836653387 / 0.7981722729924169\n",
      "Epoch 99/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0390\n",
      "\n",
      "Precision/Recall/F-score: 0.8070246265643924 / 0.7964143426294821 / 0.8016843793864047\n",
      "Epoch 100/100\n",
      "14041/14041 [==============================] - 1s 47us/step - loss: 0.0388\n",
      "\n",
      "Precision/Recall/F-score: 0.8093004283091985 / 0.7904382470119522 / 0.7997581376599819\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "cnn = Conv1D(100,3, activation='relu', padding='same')(embeddings)\n",
    "#cnn1 = Conv1D(100,3, activation='relu', padding='same')(inp)\n",
    "#dense=TimeDistributed(Dense(class_count, activation=\"softmax\"))(cnn)\n",
    "#cnn2=Conv1D(100,3, activation='relu', padding='same')(dense)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(cnn)\n",
    "model_cnn=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "lr_cnn=0.001\n",
    "batch_size_cnn=100\n",
    "epochs_cnn=100\n",
    "\n",
    "optimizer=Adam(lr=lr_cnn) # define the learning rate\n",
    "model_cnn.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "# This is our model for outputting the time step wise kernel activations.\n",
    "cnn_out_model=Model(inputs=[inp], outputs=[cnn])\n",
    "# We have to compile the model, but we nerver train it directly\n",
    "cnn_out_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\",sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist_cnn=model_cnn.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=batch_size_cnn,verbose=1,epochs=epochs_cnn, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3250/3250 [==============================] - 0s 86us/step\n",
      "Predictions shape: (3250, 113, 100)\n"
     ]
    }
   ],
   "source": [
    "input_data=validation_vectorized_data_padded\n",
    "predictions=cnn_out_model.predict(input_data, verbose=1)\n",
    "print(\"Predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings from the model: (50002, 300)\n",
      "Kernels: (3, 300, 100)\n",
      "Kernel 0:\n",
      "<OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL> | <OOV> <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 0: lastname postcode postcode [0.27328673 0.28709066 0.25920066] \n",
      "\n",
      "Kernel 1:\n",
      "WITH <OOV> IN | Vale 4 1 | Standard <OOV> on | Royals 4-0 . | winger <OOV> <OOV> | Vale 2 Oxford | Rovers 1 Stockport | Rovers 3 1 | City 0 <SPECIAL> | midfielder <OOV> .\n",
      "Hypothetical maximum activation 1: mappings Maroons 0 [5.6020603 3.7769701 3.0427697] \n",
      "\n",
      "Kernel 2:\n",
      "said his mother | had taken a | greatest gift to | said 20-year-old Ali | Bob Kennedy ( | Mohammed Idris . | said economist Lynn | had seized two | Aziz said in | said Rios .\n",
      "Hypothetical maximum activation 2: ATF Abdallah Winfrey [2.8304188 3.6919897 2.7543094] \n",
      "\n",
      "Kernel 3:\n",
      "general corporate purposes | own currency , | his family is | own territory in | their pre-war place | human rights prize | fundamental market position | European institution . | own territory . | any political statement\n",
      "Hypothetical maximum activation 3: PLUS philosophy sender [3.5944197 4.412786  6.423675 ] \n",
      "\n",
      "Kernel 4:\n",
      "hooker Sean Fitzpatrick | 250,000 award or | sales <OOV> vs | peace envoy Alexander | employment report had | Prime Minister Viktor | Income N / | 18-year-old Ian <OOV> | Eric Anthony hit | 287 Ian <OOV>\n",
      "Hypothetical maximum activation 4: Flea inches Tomas [3.6692815 4.433078  5.6271286] \n",
      "\n",
      "Kernel 5:\n",
      "the country ' | the news conference | the 35-year-old Bruno | the tourist , | the latest evidence | the conservative government | the dissident 's | the new , | the precious metal | German radio the\n",
      "Hypothetical maximum activation 5: told Allied convenience [4.0695157 4.935191  4.201167 ] \n",
      "\n",
      "Kernel 6:\n",
      "sprint champions at | champion Billy Mayfair | overnight talks to | metres champions going | metres champion Donovan | ( Joel <OOV> | about himself , | a couple of | physicist who discovered | captain Andy Townsend\n",
      "Hypothetical maximum activation 6: 2024 anchor Matisse [4.636594 5.24079  5.223852] \n",
      "\n",
      "Kernel 7:\n",
      "British Universities ( | Gloucestershire or Sussex | Glamorgan ( three | Belarus 5-1 in | Derbyshire ( three | Gloucestershire 183 and | Gloucestershire 183 and | Leicestershire ( three | Worcestershire <SPECIAL> <SPECIAL> | Turkey 2-1 <SPECIAL>\n",
      "Hypothetical maximum activation 7: stats MI5 Piraeus [4.220927  5.3973327 5.8579397] \n",
      "\n",
      "Kernel 8:\n",
      "Texas trader said | Bank labourer turned | Moscow street vendor | the centre was | of Jerusalem \" | Russian news reports | Chechnya peace plan | the South Korean | the works , | Northern Ireland 1-0\n",
      "Hypothetical maximum activation 8: Mae Kharkiv bookshop [4.9849706 5.2194223 5.817339 ] \n",
      "\n",
      "Kernel 9:\n",
      "Labor Day weekend | <OOV> <SPECIAL> <SPECIAL> | F : NA | Prix Final in | <OOV> CUP <OOV> | <OOV> CUP . | Disputes Act which | <OOV> <SPECIAL> <SPECIAL> | <OOV> AT <OOV> | <OOV> <OOV> <OOV>\n",
      "Hypothetical maximum activation 9: Spring Record E.T. [4.2843966 3.8143818 3.8611202] \n",
      "\n",
      "Kernel 10:\n",
      "reported when spot | only gradually this | said Kim had | first hole and | said Wall Street | signed by Russian | to attempt suicide | strike when he | fights before finally | up front .\n",
      "Hypothetical maximum activation 10: Uighurs moratorium everytime [5.110386  3.679086  4.7388487] \n",
      "\n",
      "Kernel 11:\n",
      "full text of | talks ) was | Der Spiegel , | market in the | market sentiment will | paper quoted one | said Hugh Johnson | house ) for | dollars ) . | pounds ) of\n",
      "Hypothetical maximum activation 11: AIP disclosures theta [4.431241  3.3824174 4.0652122] \n",
      "\n",
      "Kernel 12:\n",
      "accused Iran 's | Moslem move towards | Bahraini village , | were persecuted while | northern Iraq , | northern Iraq in | half to narrow | were forced back | a room to | long prison terms\n",
      "Hypothetical maximum activation 12: fairly Maldives northwards [5.184559  3.909954  6.1992464] \n",
      "\n",
      "Kernel 13:\n",
      "- Spartak ( | - Laurent Blanc | - Robin Brooke | - Figures are | The auto industry | - Bertrand <OOV> | - $ 389 | This division would | - Alberto Garcia | - Goran <OOV>\n",
      "Hypothetical maximum activation 13: SYDNEY Targeting kidnappings [4.3907547 6.0619717 5.144693 ] \n",
      "\n",
      "Kernel 14:\n",
      "<OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL> | <OOV> <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 14: preloadtitle postcode Userpage [0.27585858 0.28526142 0.2516097 ] \n",
      "\n",
      "Kernel 15:\n",
      "matches against the | caused outrage in | matches against <SPECIAL> | had raped the | soccer friendly on | men attacked the | which ousted her | markets weakened after | immigrants home . | immigrants home since\n",
      "Hypothetical maximum activation 15: aeroplane provokes waxing [5.0721188 4.7413692 4.7229376] \n",
      "\n",
      "Kernel 16:\n",
      "<OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL> | <OOV> <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 16: TS dossier postcode [0.31723416 0.35528794 0.31522518] \n",
      "\n",
      "Kernel 17:\n",
      "committee was set | LM Ericsson AB | chairman ) Christian | department underestimated the | Options Exchange ( | vendor Valery <OOV> | parliament and <OOV> | major telecoms carriers | committee member to | gave labour unions\n",
      "Hypothetical maximum activation 17: Eurostar subcommittee LSE [4.0373755 5.310575  4.4092374] \n",
      "\n",
      "Kernel 18:\n",
      "racing driver Jackie | , Billy Mayfair | , Dean Headley | , Colin <OOV> | ( 25th , | , Steve <SPECIAL> | St Johnstone 2 | ( 5th ) | and lap <SPECIAL> | , Dave <OOV>\n",
      "Hypothetical maximum activation 18: Gunter terminals Yates [4.367577  5.069515  7.3936596] \n",
      "\n",
      "Kernel 19:\n",
      "Bevan allowed Van | Yeltsin ordered to | Ince 's trousers | Capello last season | blasts president . | Pettitte became the | AB said on | Estes shot a | Slater run out | , fired up\n",
      "Hypothetical maximum activation 19: Deepak LFC distanced [5.645352  5.9304757 5.4562216] \n",
      "\n",
      "Kernel 20:\n",
      "Zidane 46th ) | Lewis 80 not | Lewis 94 , | Headley not out | Ahmed not out | McGrath not out | he was being | he was being | Jones & Co | astrologer was asked\n",
      "Hypothetical maximum activation 20: moths Hussain 35th [3.3304949 4.656873  4.061824 ] \n",
      "\n",
      "Kernel 21:\n",
      "Belgium beat Turkey | Slovakia beat the | Belarus beat Estonia | Britain to China | Britain , in | Lithuania - <OOV> | Lithuania - <OOV> | Britain by Richard | Spain travels to | Germany were to\n",
      "Hypothetical maximum activation 21: overviews Lithuania toolkit [3.1699767 6.5627985 4.5390673] \n",
      "\n",
      "Kernel 22:\n",
      "use German bond | beat Bernd <OOV> | conditions Swiss investors | or acquisitions . | , German and | seized by police | stormed German diplomat | beat Frederic <OOV> | security forces were | security forces .\n",
      "Hypothetical maximum activation 22: SDF thinning Aviva [5.4181995 2.9703128 5.8411927] \n",
      "\n",
      "Kernel 23:\n",
      "Atherton ( captain | Fleming c <OOV> | Magee during Wednesday | Waugh c and | Oates about his | Hill called <OOV> | Walker ( England | Little followed up | Miller successfully defended | Wild ( U.S.\n",
      "Hypothetical maximum activation 23: Waldo Hibbert inequities [6.415241  4.9291816 5.1970396] \n",
      "\n",
      "Kernel 24:\n",
      "<OOV> Corp. in | Motor Corp of | <OOV> Pacific in | <OOV> Co were | Joaquin del <OOV> | Cricket Board <SPECIAL> | Gold Fields of | street vendor stabbed | Diego Padres blanked | Standard <OOV> on\n",
      "Hypothetical maximum activation 24: FC Sao Aluminium [6.3698883 3.9214528 5.3431854] \n",
      "\n",
      "Kernel 25:\n",
      "state-owned press distribution | distribution networks . | 's fear of | is that if | 's press distribution | located throughout the | said consumer spending | of voter registration | on international law | one-day international (\n",
      "Hypothetical maximum activation 25: flea ostrich confidentiality [4.4132266 5.07011   5.453115 ] \n",
      "\n",
      "Kernel 26:\n",
      "grand slam with | first victory under | grand slam and | last month to | last month to | levels before the | went straight . | election win would | World Cup qualifying | gains as Tokyo\n",
      "Hypothetical maximum activation 26: byproducts escort sparingly [5.174669  4.2301826 5.3545094] \n",
      "\n",
      "Kernel 27:\n",
      "<OOV> walked free | Malik c Stewart | Atherton ( captain | Mordecai singled , | Fleming c <OOV> | Waugh c and | Miller successfully defended | Bevan c <OOV> | Nicol of Scotland | <OOV> scattered seven\n",
      "Hypothetical maximum activation 27: Fabio Mayfair sped [7.3745966 4.9873633 5.303821 ] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel 28:\n",
      "allowing a hit | leader Aslan <OOV> | leader Aslan <OOV> | leader Aslan <OOV> | yielding a single | after the break | after the break | arrested the militants | said a few | where a majority\n",
      "Hypothetical maximum activation 28: foreigner king brand-new [5.297423  5.355601  4.0663314] \n",
      "\n",
      "Kernel 29:\n",
      "<OOV> <SPECIAL> <SPECIAL> | <OOV> <SPECIAL> <SPECIAL> | <OOV> <SPECIAL> <SPECIAL> | <OOV> <SPECIAL> <SPECIAL> | <OOV> <SPECIAL> <SPECIAL> | <OOV> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL> | <OOV> 4 2 | <OOV> - <OOV>\n",
      "Hypothetical maximum activation 29: Referencing VIDEO VIDEO [0.27015433 0.26864776 0.26402205] \n",
      "\n",
      "Kernel 30:\n",
      "agreement \" shall | Olympic champion Lindsay | prefix number denotes | prefix number denotes | prefix number denotes | prefix number denotes | markets weakened after | demonstrations by students | demonstrations by students | telecoms group LM\n",
      "Hypothetical maximum activation 30: Minister licences introduction [6.0437584 5.454273  6.111915 ] \n",
      "\n",
      "Kernel 31:\n",
      "Algiers late on | downtown area . | stadium lights are | Bahraini village . | Stansted airport , | Stansted airport on | Tianjin city in | Dutch campsite . | Venice festival . | : Nottinghamshire 214\n",
      "Hypothetical maximum activation 31: near Located Brunswick [4.8416576 2.934362  4.961891 ] \n",
      "\n",
      "Kernel 32:\n",
      "his testimony . | by security officers | his 121 RBI | 58 1 49 | 58 1 49 | by 213 , | poisoned 157 , | by $ 6.9 | by methanol poisoning | 72 73 67\n",
      "Hypothetical maximum activation 32: DM achievements Benzema [4.278395  5.2299876 4.1772223] \n",
      "\n",
      "Kernel 33:\n",
      "paper quoted one | independence bid . | loss $ <OOV> | loss $ <OOV> | , Fax <OOV> | issue price of | reports implied the | <OOV> refinancing rate | , instrument panels | , ISDN equipment\n",
      "Hypothetical maximum activation 33: Ukip में Countrywide [3.536431  3.3669784 4.94026  ] \n",
      "\n",
      "Kernel 34:\n",
      "Rochdale 0 <SPECIAL> | Hull 0 Barnet | Hartlepool 0 <SPECIAL> | Falkirk 0 <SPECIAL> | Blackpool 0 <OOV> | Roe 69 71 | Morris 69 ; | Berwick 0 <SPECIAL> | Rovers 3 1 | Grimsby 0 Portsmouth\n",
      "Hypothetical maximum activation 34: chelsea Lazio Bayern [3.990399  6.620867  5.4576674] \n",
      "\n",
      "Kernel 35:\n",
      "<OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL> | <OOV> <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 35: dossier olds dossier [1.2268119 0.9121931 1.157726 ] \n",
      "\n",
      "Kernel 36:\n",
      "search on Thursday | as he was | markets any time | market does <OOV> | minutes there were | variables that most | percent creates psychological | spending that some | market where demand | situation there ,\n",
      "Hypothetical maximum activation 36: unsecured memberships Claire [4.7001977 3.866683  5.3973446] \n",
      "\n",
      "Kernel 37:\n",
      "Mauro Silva from | Mauro Silva for | Tom Johnson successfully | Paul <OOV> was | Kenny Harrison and | Nate Miller a | Phil Simmons took | Matt Lawton hit | Dirk <OOV> from | Dennis Mitchell <OOV>\n",
      "Hypothetical maximum activation 37: prodigy Andreas Qaddafi [5.6533647 6.0604105 5.736485 ] \n",
      "\n",
      "Kernel 38:\n",
      "international at <OOV> | products in favor | international efforts aimed | any unrest as | to 29 matches | by 29 percent | cricket match between | economic activity . | political agreement to | detained 26 suspected\n",
      "Hypothetical maximum activation 38: top-level non-religious demonstration [5.053619  5.1719604 5.3881197] \n",
      "\n",
      "Kernel 39:\n",
      "Securities Inc . | Rights National Observatory | Purchasing Management ( | and Telecommunications Administration | Monetary Fund might | Agriculture Department officials | System Inc. and | Hizbollah guerrillas will | and Democracy in | Administration Director <OOV>\n",
      "Hypothetical maximum activation 39: Insurance Functions Packet [5.7605023 5.9707384 5.464829 ] \n",
      "\n",
      "Kernel 40:\n",
      "Arafat said the | Aziz said on | Ahmed not out | Ahmed 46 <SPECIAL> | Salim Malik , | joked that his | be fooled by | Ahmed 79 ; | Abdullah <OOV> , | Arafat arrived in\n",
      "Hypothetical maximum activation 40: bad Salman infertile [4.6195736 3.6461923 3.0408454] \n",
      "\n",
      "Kernel 41:\n",
      "jailing nearly 200 | Gough 4 <SPECIAL> | Gough 33 <SPECIAL> | had said Yeltsin | $ 31 a | detaining Wang on | analysts said . | killed accidentally by | met last January | McGrath 50 <SPECIAL>\n",
      "Hypothetical maximum activation 41: FAQs Malik ex-husband [4.0353255 3.0783548 3.1078556] \n",
      "\n",
      "Kernel 42:\n",
      "$ 28,000 fine | million 680 million | only 0.2 percent | goal . \" | behaviour . <SPECIAL> | $ 386 - | $ 1,000 bond | phone lines by | a million people | match which his\n",
      "Hypothetical maximum activation 42: standstill solvency 457 [5.2146378 6.012945  6.313536 ] \n",
      "\n",
      "Kernel 43:\n",
      "Iraq , where | Iraq , U.S. | Iraq , but | civilians had been | , winning percentage | , winning percentage | , winning percentage | , winning percentage | qualifier on Saturday | checkpoint on the\n",
      "Hypothetical maximum activation 43: mi def justifying [3.821872 4.558928 4.229739] \n",
      "\n",
      "Kernel 44:\n",
      "session low of | prices recovered from | a moderate voice | earnings cut into | employment report , | earnings fell in | may cut rates | AT TOP AFTER | futures finished a | market prices .\n",
      "Hypothetical maximum activation 44: WORLD olds scouting [5.7927585 3.8880167 4.62972  ] \n",
      "\n",
      "Kernel 45:\n",
      "v Scotland <SPECIAL> | long-range penalties . | ) 6-0 7-6 | 67 78 <SPECIAL> | framework political agreement | runs away from | ( Yugoslavia ) | ( Yugoslavia ) | ( Yugoslavia ) | index had earlier\n",
      "Hypothetical maximum activation 45: six-month dissociation SSI [4.138638  2.7327957 4.5320673] \n",
      "\n",
      "Kernel 46:\n",
      "reports implied the | minutes earlier , | minutes later when | minutes later and | newspaper he thought | markets weakened after | rates up to | medicines from the | goals as 1998 | match as the\n",
      "Hypothetical maximum activation 46: Defensive monopolies repeats [4.343309  4.2489634 5.5329847] \n",
      "\n",
      "Kernel 47:\n",
      "Rovers 1 Stockport | 4 2 1 | Rovers 3 1 | team mate Les | group LM Ericsson | 4 1 1 | 3 2 1 | 4 2 1 | 4 1 1 | 3 1 1\n",
      "Hypothetical maximum activation 47: Nestlé AGM untrue [5.6555343 3.6518729 4.2757688] \n",
      "\n",
      "Kernel 48:\n",
      "Colin Montgomerie 68 | - Raul Rodrigo | - Ian Healy | - Enzo <OOV> | Nate Miller successfully | Neal Lancaster , | captain Dean Jones | champ Payne Stewart | - Felix <OOV> | Tom Moody took\n",
      "Hypothetical maximum activation 48: Eight GK Pistorius [5.059239  4.4909263 6.4709845] \n",
      "\n",
      "Kernel 49:\n",
      "YORK 75 59 | YORK 4 <SPECIAL> | YORK 74 59 | YORK 2 <SPECIAL> | YORK 59 76 | YORK 59 75 | YORK <SPECIAL> <SPECIAL> | YORK <SPECIAL> <SPECIAL> | YORK AT <OOV> | YORK AT <OOV>\n",
      "Hypothetical maximum activation 49: NEW CITY 255 [5.511115 4.559862 3.789937] \n",
      "\n",
      "Kernel 50:\n",
      "Gough 33 <SPECIAL> | Ponting not out | Anthony Washington ( | Arafat would hold | Ali Ahmed from | son beside her | McGrath 50 <SPECIAL> | me up , | Rolf Sorensen ( | Bob Kennedy (\n",
      "Hypothetical maximum activation 50: shot Muhammad accompanying [3.975215  4.1584444 4.1306844] \n",
      "\n",
      "Kernel 51:\n",
      "James Dalton , | Spartak ( to | Giovanni ( 14th | Michael Jones ( | Laurent Blanc , | Nico Van <OOV> | Bay Co <OOV> | Lindsay Davenport ( | played , won | played , won\n",
      "Hypothetical maximum activation 51: - Blasio MAY [5.2831135 4.961599  6.078721 ] \n",
      "\n",
      "Kernel 52:\n",
      "ANGELES AT <OOV> | ANGELES AT <OOV> | <OOV> 58 74 | <OOV> 57 74 | <OOV> AT NEW | <OOV> AT <OOV> | <OOV> AT NEW | <OOV> AT <OOV> | Wall Street economists | <OOV> 9 <SPECIAL>\n",
      "Hypothetical maximum activation 52: SM Reuters Palomar [5.2864866 6.542395  4.7763963] \n",
      "\n",
      "Kernel 53:\n",
      "<OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL> | <OOV> <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 53: USER 2601 Userpage [0.26008788 0.22082075 0.24124272] \n",
      "\n",
      "Kernel 54:\n",
      "border . <SPECIAL> | 2,000 . <SPECIAL> | figure <OOV> to | points . <SPECIAL> | just outside the | lives . <SPECIAL> | sovereignty . <SPECIAL> | will meet at | forces . <SPECIAL> | militants . <SPECIAL>\n",
      "Hypothetical maximum activation 54: spanking launchers Sargent [5.9044075 4.4673104 3.5849988] \n",
      "\n",
      "Kernel 55:\n",
      "four minutes earlier | 54 minutes . | 13 minutes , | 10 minutes only | five minutes there | five minutes to | two blocks from | planned things so | 15 minutes by | possible within the\n",
      "Hypothetical maximum activation 55: Planck summarised MPC [5.4056406 3.1778493 5.356807 ] \n",
      "\n",
      "Kernel 56:\n",
      "OB 0 <SPECIAL> | Samsung 0 <SPECIAL> | Darlington 2 <SPECIAL> | Hartlepool 0 <SPECIAL> | Cardiff 2 <SPECIAL> | Watford 2 <SPECIAL> | Stoke 2 <SPECIAL> | Exeter 0 <SPECIAL> | Falkirk 0 <SPECIAL> | Hamilton 0 <SPECIAL>\n",
      "Hypothetical maximum activation 56: Fabregas Cruises ACL [4.417675  5.535007  4.8883557] \n",
      "\n",
      "Kernel 57:\n",
      "an ounce range | rise forecast by | this premium , | a minute before | in assets , | in assets , | in assets , | really levels the | could win in | would cost <OOV>\n",
      "Hypothetical maximum activation 57: U.S mildly granularity [5.1517243 4.241758  4.6274796] \n",
      "\n",
      "Kernel 58:\n",
      "Caucasus region . | Iraqi city of | Tripoli airport which | Football Union , | Korea return a | China coast ; | Tripoli for celebrations | OSCE principles , | Guangdong province of | OSCE Chechnya mission\n",
      "Hypothetical maximum activation 58: Woodford CAR Leviathan [5.17937   3.412216  5.3801193] \n",
      "\n",
      "Kernel 59:\n",
      "extended their first | , Manchester ) | , Lancashire 218 | , Manchester City | said Texas manager | but his doctors | hit the road | that could have | said the market | , under the\n",
      "Hypothetical maximum activation 59: schemas paradoxically Lancashire [4.8997054 4.1804347 5.531209 ] \n",
      "\n",
      "Kernel 60:\n",
      "prescription pharmaceutical products | savings rate since | paid $ 1,500 | treated twice last | international treaty which | cut rates . | inhuman conditions and | prison terms . | global sales of | talked about the\n",
      "Hypothetical maximum activation 60: crafted testimonials loans [3.8427432 5.152317  6.4935913] \n",
      "\n",
      "Kernel 61:\n",
      "<OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL> | <OOV> <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 61: profiling VIDEO MATLAB [0.2887676  0.2509649  0.28558847] \n",
      "\n",
      "Kernel 62:\n",
      "stock market will | ) : <SPECIAL> | ) : <SPECIAL> | blanked the Kansas | merger makes us | bank assets July | added : \" | ) : Yorkshire | have dropped five | said it killed\n",
      "Hypothetical maximum activation 62: CRS payrolls Centrica [2.3347201 2.4470658 2.4099238] \n",
      "\n",
      "Kernel 63:\n",
      "Deportivo chairman Augusto | soccer federation dissolved | 's auto safety | club rugby in | police suspected her | closes university after | largest militant group | soccer matches <SPECIAL> | 's political status | chairman ) Christian\n",
      "Hypothetical maximum activation 63: Singapore Olympiacos federation [5.818504  4.0247784 4.7001534] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel 64:\n",
      "( Australia ) | b Gough 0 | ( Ireland ) | ) <SPECIAL> <SPECIAL> | : Sussex 363 | : Sussex 363 | ) <SPECIAL> <SPECIAL> | ( Australia ) | ( 2nd minute | ( Ireland )\n",
      "Hypothetical maximum activation 64: quilts Drivers Collector [5.6926985 4.4995947 5.215971 ] \n",
      "\n",
      "Kernel 65:\n",
      "<OOV> Aziz said | Mohammed Idris . | Chris Lewis , | C. Lewis 80 | C. Lewis 94 | 20-year-old Ali Ahmed | be replaced by | <OOV> wa <OOV> | of Islam group | president Mohamed <OOV>\n",
      "Hypothetical maximum activation 65: Minister Hariri Idris [4.462101  3.0111177 5.032946 ] \n",
      "\n",
      "Kernel 66:\n",
      "6 - 2 | the final day | 386 - $ | Monday due to | day I 've | jump competition at | 418 - $ | live in peace | day . <SPECIAL> | number denotes seeding\n",
      "Hypothetical maximum activation 66: zip M6 proofreading [5.510318  3.4981873 4.8146358] \n",
      "\n",
      "Kernel 67:\n",
      "Ali Ahmed from | Anthony Washington ( | Gough 33 <SPECIAL> | Armando Alvarez was | Ponting not out | Bevan allowed Van | Anthony Hill ( | Fleming 44 <SPECIAL> | Rhodes 57 ; | Duran still talks\n",
      "Hypothetical maximum activation 67: exclaimed Sanjay nears [4.157566  3.6343367 4.3144674] \n",
      "\n",
      "Kernel 68:\n",
      "fined 25,000 Swiss | - $ 380 | <OOV> to war | - Belgian King | fined $ 1,000 | <OOV> OF <OOV> | throwing beer on | England this summer | <OOV> IN <OOV> | F / <OOV>\n",
      "Hypothetical maximum activation 68: 2009-2010 BRUSSELS Portage [3.3646724 1.9934896 3.7745547] \n",
      "\n",
      "Kernel 69:\n",
      "deported thousands of | composite index closed | said Thatcher 's | captain David Platt | president Fidel Ramos | excused his absence | broke Ruben Sierra | Jones index closed | leader Rami <OOV> | president Ronald Reagan\n",
      "Hypothetical maximum activation 69: Moyer pronouncement Amos [5.715263  5.3465276 4.67886  ] \n",
      "\n",
      "Kernel 70:\n",
      "DETROIT <SPECIAL> <SPECIAL> | DETROIT <SPECIAL> <SPECIAL> | <OOV> ) <SPECIAL> | SRI <OOV> . | SRI <OOV> <OOV> | <OOV> ) , | <OOV> ) ; | 75 ) , | CHICAGO <SPECIAL> <SPECIAL> | CHICAGO <SPECIAL> <SPECIAL>\n",
      "Hypothetical maximum activation 70: proudly BRUSSELS ---- [7.266692  3.7342415 4.895273 ] \n",
      "\n",
      "Kernel 71:\n",
      "Armando Alvarez was | Mike Conley ( | Rodrigo Lara ( | Der Spiegel he | Michelle Freeman ( | Nasser Hussain and | Mauro Silva from | Mauro Silva for | Tristan Hoffman ( | Andy Townsend ,\n",
      "Hypothetical maximum activation 71: criticized Kamal Sha [5.4802256 5.658097  4.77767  ] \n",
      "\n",
      "Kernel 72:\n",
      "$ 4 billion | Spartak 3 1 | 2-0 WIN . | 5-0 WIN . | Hyundai 6 <OOV> | $ 200 million | LG 2 OB | Watford 4 2 | Hyundai 2 <SPECIAL> | Samsung 49 5\n",
      "Hypothetical maximum activation 72: SPSS FC PDP [2.670165  4.0703464 3.8956757] \n",
      "\n",
      "Kernel 73:\n",
      "newspapers reported on | also reported Canada | prices recovered from | tabloid reported he | companies rose in | market in the | ) beat Andrei | ) beat Christian | ) beat Dan | ) beat 4\n",
      "Hypothetical maximum activation 73: buys RAC heavy [3.914059  3.8318148 4.340068 ] \n",
      "\n",
      "Kernel 74:\n",
      "and demolished the | and booed the | vendor stabbed to | quarter finals ( | to victory in | in vain to | and Iraqi tanks | , I knew | soldier yelled at | in court on\n",
      "Hypothetical maximum activation 74: conqueror orders blitz [4.739218  5.067989  4.8604555] \n",
      "\n",
      "Kernel 75:\n",
      ") 1- 6 | ) 6 - | ) 6-7 ( | ) 3-6 6-3 | ) 1-6 <OOV> | ) four minutes | - 1 - | ) 4-6 6-2 | ) Motorola 2 | ) 4-6 6\n",
      "Hypothetical maximum activation 75: Safina ISBN Ensure [4.5278964 3.3017087 5.4670787] \n",
      "\n",
      "Kernel 76:\n",
      "team record of | and Cubs split | after games played | after games played | remained on the | crackdown on bullion | in stocks during | Friday expiry in | July employment report | spokesman on Saturday\n",
      "Hypothetical maximum activation 76: FF streak specs [4.9695334 3.7745154 4.464736 ] \n",
      "\n",
      "Kernel 77:\n",
      "Torrance <OOV> ( | Felicia <OOV> ( | DETROIT <OOV> <SPECIAL> | Louis <OOV> , | Michael <OOV> ( | DETROIT <SPECIAL> <SPECIAL> | DETROIT <SPECIAL> <SPECIAL> | Erik <OOV> ( | Florian <OOV> ( | North accepted Seoul\n",
      "Hypothetical maximum activation 77: Eldorado Nordstrom tweet [5.44415  5.977628 3.561625] \n",
      "\n",
      "Kernel 78:\n",
      "Johansson ( Sweden | Montgomerie 68 76 | Atherton ( captain | McGregor ( Britain | Kennedy ( U.S. | Conley ( U.S. | Townsend ( 5th | Adkins ( U.S. | Martin ( U.S. | Melville ( 33rd\n",
      "Hypothetical maximum activation 78: Johnny SPSS 6-3 [6.8442273 4.6550717 3.861054 ] \n",
      "\n",
      "Kernel 79:\n",
      "U.S. economic data | Moldova 2-0 ( | Moldova on Sunday | Liechtenstein 5-0 ( | U.S. shrink . | U.S. president Ronald | Iraq ordered many | U.S. debt futures | U.S. debt futures | U.S. warplanes ,\n",
      "Hypothetical maximum activation 79: muslim Ethiopia cloned [4.5839477 6.698755  4.0469413] \n",
      "\n",
      "Kernel 80:\n",
      "over Seattle in | about sums up | , innings closed | a session low | landed him in | to score the | to bat in | up bidding for | a prayer . | a checkpoint on\n",
      "Hypothetical maximum activation 80: IDC mails quests [5.2886386 3.41253   4.6789894] \n",
      "\n",
      "Kernel 81:\n",
      "Gold Fields of | DETROIT <SPECIAL> <SPECIAL> | DETROIT <SPECIAL> <SPECIAL> | & Sons . | Mark Butcher who | Nate Miller a | Controls Inc and | Carlos saved the | Al freighter which | & Co Inc\n",
      "Hypothetical maximum activation 81: DRS TOKYO Moores [5.737952  3.9787703 6.0949845] \n",
      "\n",
      "Kernel 82:\n",
      "strike when he | said consumer spending | whether Texas billionaire | round Linda Wild | captain Dean Jones | said after hearing | Thursday when a | Thursday with 80 | Monday when an | , Ricky <SPECIAL>\n",
      "Hypothetical maximum activation 82: CAC directorate Cass [4.8756394 4.0426054 5.2514977] \n",
      "\n",
      "Kernel 83:\n",
      ") beat <OOV> | ( Cyril <OOV> | ( Cyril <OOV> | LM Ericsson AB | ) beat 17 | ) beat <OOV> | ) beat Roberto | ) beat <OOV> | Copper Ltd , | assets July June\n",
      "Hypothetical maximum activation 83: brokerage LFC sam [4.848893  3.8110611 4.7584143] \n",
      "\n",
      "Kernel 84:\n",
      "Pharmaceuticals Inc of | Data Services said | & Co Inc | Controls Inc and | destroys restaurant in | build units for | Chemical Co of | Mining Co this | Company Ltd , | & Co .\n",
      "Hypothetical maximum activation 84: J.J. BN statement [9.113027 4.948887 6.726396] \n",
      "\n",
      "Kernel 85:\n",
      "savings rate since | goal . <SPECIAL> | points . <SPECIAL> | quarter . <SPECIAL> | try . <SPECIAL> | runs . <SPECIAL> | straight . <SPECIAL> | set . <SPECIAL> | half . <SPECIAL> | days . <SPECIAL>\n",
      "Hypothetical maximum activation 85: fifth enforcer receivers [4.2003865 2.7826223 4.1134405] \n",
      "\n",
      "Kernel 86:\n",
      "doubled over the | apparently inhabited only | reduced by 29 | affected by the | fixed yet for | unprotected against criminal | battled against nationalist | quoted at an | set up at | a team )\n",
      "Hypothetical maximum activation 86: exhaustively bolstered seabed [4.332419  4.096254  4.9199247] \n",
      "\n",
      "Kernel 87:\n",
      "Capello last season | Treasury securities were | Barnet 0 <SPECIAL> | Rochdale 0 <SPECIAL> | Bank settlements surrounding | Moscow a week | Bank as a | Fleming 0 <SPECIAL> | States last week | SPC group and\n",
      "Hypothetical maximum activation 87: chip Causeway step-by-step [5.5742855 5.49418   5.3710823] \n",
      "\n",
      "Kernel 88:\n",
      "St Pauli 3 | St Mirren 3 | <OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> -\n",
      "Hypothetical maximum activation 88: postcode addictive postcode [1.3670408 1.603381  1.3115472] \n",
      "\n",
      "Kernel 89:\n",
      "games against India | prices lingered at | overs against Sri | earnings fell in | prices fell on | qualifier against Scotland | earnings cut into | miners have been | rebels against government | rigs under contract\n",
      "Hypothetical maximum activation 89: sub equivalents shields [5.5784764 4.0458207 5.0510287] \n",
      "\n",
      "Kernel 90:\n",
      "<SPECIAL> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 90: profiling profiling InternetArchiveBot [0.35206193 0.28026715 0.24780287] \n",
      "\n",
      "Kernel 91:\n",
      "Headley not out | <OOV> become possible | Ponting not out | Alan is the | Pettitte became the | has joined <OOV> | Penney of the | Real with a | Platt are both | Ahmed from <OOV>\n",
      "Hypothetical maximum activation 91: Skip Kiran publishes [4.544509  2.914012  4.7281528] \n",
      "\n",
      "Kernel 92:\n",
      "<SPECIAL> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 92: Tsarnaev postcode Referencing [0.2965304  0.28003892 0.28771704] \n",
      "\n",
      "Kernel 93:\n",
      "Salim Malik , | Ali Ahmed from | Aziz says Baghdad | Dale 69 , | Lewis 94 , | Gough 33 <SPECIAL> | Ali at the | Rami <OOV> in | Butcher 70 , | Johnson 84 ;\n",
      "Hypothetical maximum activation 93: interviewing Malik nears [4.705592  3.7735777 4.013824 ] \n",
      "\n",
      "Kernel 94:\n",
      "bond interest rates | said Iranian secret | on international law | his international competitive | about 1,400 civilians | 's foreign ministry | activities hostile to | other right-wing whites | quoted military officials | sponsor international terrorism\n",
      "Hypothetical maximum activation 94: pre-tax lines foreign [4.4255996 4.898413  5.7559333] \n",
      "\n",
      "Kernel 95:\n",
      "<OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL> | <OOV> <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 95: emulator postcode preloadtitle [0.217478   0.27713367 0.24275449] \n",
      "\n",
      "Kernel 96:\n",
      "<OOV> - <OOV> | <OOV> <OOV> <SPECIAL> | Canada beat Panama | <OOV> : <SPECIAL> | Canada - <OOV> | Panama - Jorge | Attendance : <OOV> | <OOV> <OOV> - | Andy <OOV> <SPECIAL> | <OOV> <OOV> <SPECIAL>\n",
      "Hypothetical maximum activation 96: dossier dossier profiling [2.4055614 2.0620308 2.5043125] \n",
      "\n",
      "Kernel 97:\n",
      "Jamil said . | Jordan said in | Jordan said . | Jordan said . | <SPECIAL> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL> | <SPECIAL> <SPECIAL> <SPECIAL>\n",
      "Hypothetical maximum activation 97: profiling profiling Cricinfo [2.8379252 2.0760283 1.9176455] \n",
      "\n",
      "Kernel 98:\n",
      "- $ <OOV> | ( $ <OOV> | ( $ 2,000 | - $ 389 | ( $ <OOV> | ( $ 1 | ( $ <OOV> | - $ 380 | by $ 6.9 | : $ <OOV>\n",
      "Hypothetical maximum activation 98: naturalization LONDON $ [3.1369343 5.0862494 4.8901954] \n",
      "\n",
      "Kernel 99:\n",
      "product prices lingered | 70 metres ) | bonded zone in | over 1,000 people | by $ 6.9 | invest $ 200 | paid $ 1,500 | three km ( | over $ 7 | a $ 1\n",
      "Hypothetical maximum activation 99: generalizing profiling NIS [5.5842786 5.008534  6.651422 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_size=3\n",
    "\n",
    "\n",
    "word_embeddings = model.layers[1].get_weights()[0]\n",
    "print(\"Word embeddings from the model:\", word_embeddings.shape)\n",
    "print(\"Kernels:\", model.layers[2].get_weights()[0].shape)\n",
    "for kernel_index in range(model.layers[2].get_weights()[0].shape[-1]):\n",
    "    kernel = model.layers[2].get_weights()[0][:,:,kernel_index] + model.layers[2].get_weights()[1][kernel_index]\n",
    "\n",
    "    # Hypothetical highest activations\n",
    "    activations = numpy.dot(kernel, word_embeddings.T)\n",
    "    best_word_indices = numpy.argmax(activations, axis=-1)\n",
    "    \n",
    "    # Highest activations seen in the validation data\n",
    "    max_time_steps = numpy.argmax(predictions[:,:,kernel_index], axis=-1)\n",
    "    max_activations = numpy.max(predictions[:,:,kernel_index], axis=-1)\n",
    "    best_sentences = numpy.argsort(-max_activations)\n",
    "    \n",
    "    best_ngrams = [input_data[best_sentences[nth]][max_time_steps[best_sentences[nth]]:max_time_steps[best_sentences[nth]]+window_size] for nth in range(10)]\n",
    "    best_ngrams = [' '.join([inversed_vocabulary[i] for i in best]) for best in best_ngrams]\n",
    "    best_ngrams = ' | '.join(best_ngrams)\n",
    " \n",
    "    print('Kernel %s:' % kernel_index)\n",
    "    print(best_ngrams)\n",
    "    print('Hypothetical maximum activation %s:' % kernel_index, ' '.join([inversed_vocabulary[wi] for wi in best_word_indices]), numpy.max(activations, axis=-1), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.01, Batch size: 100, Epochs: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Learning rate: {}, Batch size: {}, Epochs: {}'.format(lr_cnn, batch_size_cnn, epochs_cnn))\n",
    "plt.plot(hist_cnn.history['loss'])\n",
    "#plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss, final: {}'.format(round(hist_cnn.history['loss'][-1], 3)))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 113, 100)          160400    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,161,404\n",
      "Trainable params: 160,804\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 4.6544\n",
      "\n",
      "Precision/Recall/F-score: 0.5072115384615384 / 0.04203187250996016 / 0.07763061074319352\n",
      "Epoch 2/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 2.8454\n",
      "\n",
      "Precision/Recall/F-score: 0.405982905982906 / 0.3595617529880478 / 0.38136488485104586\n",
      "Epoch 3/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.9274\n",
      "\n",
      "Precision/Recall/F-score: 0.5948215535339398 / 0.5079681274900398 / 0.5479746427420221\n",
      "Epoch 4/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.6365\n",
      "\n",
      "Precision/Recall/F-score: 0.6419691695673794 / 0.5143426294820718 / 0.5711125857111259\n",
      "Epoch 5/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.4358\n",
      "\n",
      "Precision/Recall/F-score: 0.5988206201255469 / 0.6270916334661355 / 0.6126301449839447\n",
      "Epoch 6/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.2998\n",
      "\n",
      "Precision/Recall/F-score: 0.6682085786375105 / 0.6330677290836654 / 0.650163666121113\n",
      "Epoch 7/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.2636\n",
      "\n",
      "Precision/Recall/F-score: 0.6585875470121186 / 0.6278884462151394 / 0.6428717111972261\n",
      "Epoch 8/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.1887\n",
      "\n",
      "Precision/Recall/F-score: 0.6842221730471343 / 0.6159362549800796 / 0.6482859838557501\n",
      "Epoch 9/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.1616\n",
      "\n",
      "Precision/Recall/F-score: 0.7239619577824171 / 0.6217131474103585 / 0.6689529525238451\n",
      "Epoch 10/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.1111\n",
      "\n",
      "Precision/Recall/F-score: 0.600388486667844 / 0.6772908366533864 / 0.636525320602827\n",
      "Epoch 11/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.0879\n",
      "\n",
      "Precision/Recall/F-score: 0.685200668896321 / 0.652988047808765 / 0.6687066503467972\n",
      "Epoch 12/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.0771\n",
      "\n",
      "Precision/Recall/F-score: 0.71953125 / 0.550398406374502 / 0.6237020316027089\n",
      "Epoch 13/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.0519\n",
      "\n",
      "Precision/Recall/F-score: 0.7392216266604521 / 0.6318725099601593 / 0.6813446461174953\n",
      "Epoch 14/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.0278\n",
      "\n",
      "Precision/Recall/F-score: 0.7319756554307116 / 0.6229083665338645 / 0.6730520878174774\n",
      "Epoch 15/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9833\n",
      "\n",
      "Precision/Recall/F-score: 0.7104765903842116 / 0.6741035856573705 / 0.6918123275068997\n",
      "Epoch 16/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9688\n",
      "\n",
      "Precision/Recall/F-score: 0.7180885882609644 / 0.6555776892430278 / 0.6854108091221492\n",
      "Epoch 17/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9617\n",
      "\n",
      "Precision/Recall/F-score: 0.653972903698279 / 0.7115537848605578 / 0.6815493226483496\n",
      "Epoch 18/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9505\n",
      "\n",
      "Precision/Recall/F-score: 0.6618189266693978 / 0.6436254980079681 / 0.6525954352656029\n",
      "Epoch 19/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9289\n",
      "\n",
      "Precision/Recall/F-score: 0.7089776632302406 / 0.6575697211155378 / 0.6823067383216205\n",
      "Epoch 20/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8814\n",
      "\n",
      "Precision/Recall/F-score: 0.6470588235294118 / 0.6683266932270916 / 0.6575208231259186\n",
      "Epoch 21/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8737\n",
      "\n",
      "Precision/Recall/F-score: 0.710055389859395 / 0.6639442231075697 / 0.6862260654725139\n",
      "Epoch 22/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8724\n",
      "\n",
      "Precision/Recall/F-score: 0.7170984455958549 / 0.6892430278884463 / 0.7028948704926359\n",
      "Epoch 23/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8333\n",
      "\n",
      "Precision/Recall/F-score: 0.7479892761394102 / 0.6669322709163347 / 0.70513900589722\n",
      "Epoch 24/100\n",
      "14041/14041 [==============================] - 26s 2ms/step - loss: 0.9028\n",
      "\n",
      "Precision/Recall/F-score: 0.6401057401812689 / 0.6752988047808764 / 0.6572314850717332\n",
      "Epoch 25/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9704\n",
      "\n",
      "Precision/Recall/F-score: 0.6593673965936739 / 0.7017928286852589 / 0.6799189423911994\n",
      "Epoch 26/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8998\n",
      "\n",
      "Precision/Recall/F-score: 0.7122021364009861 / 0.6906374501992032 / 0.7012540453074434\n",
      "Epoch 27/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8996\n",
      "\n",
      "Precision/Recall/F-score: 0.71570492496301 / 0.6745019920318726 / 0.6944928725258949\n",
      "Epoch 28/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8878\n",
      "\n",
      "Precision/Recall/F-score: 0.7260909935004642 / 0.6231075697211156 / 0.6706689536878215\n",
      "Epoch 29/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9064\n",
      "\n",
      "Precision/Recall/F-score: 0.7184769038701623 / 0.6878486055776892 / 0.7028292285772441\n",
      "Epoch 30/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8674\n",
      "\n",
      "Precision/Recall/F-score: 0.7375545851528384 / 0.6729083665338645 / 0.70375\n",
      "Epoch 31/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8400\n",
      "\n",
      "Precision/Recall/F-score: 0.6382300884955753 / 0.7183266932270916 / 0.6759137769447049\n",
      "Epoch 32/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8216\n",
      "\n",
      "Precision/Recall/F-score: 0.7004692919812283 / 0.6838645418326693 / 0.6920673319221852\n",
      "Epoch 33/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7991\n",
      "\n",
      "Precision/Recall/F-score: 0.7417114695340502 / 0.6595617529880478 / 0.6982285955293126\n",
      "Epoch 34/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7954\n",
      "\n",
      "Precision/Recall/F-score: 0.7089447236180905 / 0.702589641434263 / 0.7057528764382192\n",
      "Epoch 35/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7888\n",
      "\n",
      "Precision/Recall/F-score: 0.7167523124357656 / 0.6946215139442231 / 0.7055134041476986\n",
      "Epoch 36/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7655\n",
      "\n",
      "Precision/Recall/F-score: 0.7094580671749433 / 0.6858565737051793 / 0.6974577129545224\n",
      "Epoch 37/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7544\n",
      "\n",
      "Precision/Recall/F-score: 0.7425910245554614 / 0.698804780876494 / 0.7200328407224958\n",
      "Epoch 38/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7894\n",
      "\n",
      "Precision/Recall/F-score: 0.7114624505928854 / 0.6454183266932271 / 0.6768330896177146\n",
      "Epoch 39/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8298\n",
      "\n",
      "Precision/Recall/F-score: 0.6543141592920354 / 0.7069721115537848 / 0.6796246648793565\n",
      "Epoch 40/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7741\n",
      "\n",
      "Precision/Recall/F-score: 0.7378707748907795 / 0.6392430278884462 / 0.6850250827196072\n",
      "Epoch 41/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7663\n",
      "\n",
      "Precision/Recall/F-score: 0.6768374592208789 / 0.702589641434263 / 0.6894731697781253\n",
      "Epoch 42/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7136\n",
      "\n",
      "Precision/Recall/F-score: 0.6876484560570071 / 0.6920318725099601 / 0.6898332009531374\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7082\n",
      "\n",
      "Precision/Recall/F-score: 0.7523830636222567 / 0.6760956175298805 / 0.7122022872731089\n",
      "Epoch 44/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7151\n",
      "\n",
      "Precision/Recall/F-score: 0.7079789388416363 / 0.6964143426294821 / 0.7021490259088171\n",
      "Epoch 45/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7413\n",
      "\n",
      "Precision/Recall/F-score: 0.7397660818713451 / 0.7055776892430279 / 0.7222675367047308\n",
      "Epoch 46/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.6989\n",
      "\n",
      "Precision/Recall/F-score: 0.7035196687370601 / 0.6768924302788845 / 0.6899492385786802\n",
      "Epoch 47/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.3668\n",
      "\n",
      "Precision/Recall/F-score: 0.6510753810816454 / 0.6211155378486056 / 0.6357426852890202\n",
      "Epoch 48/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.1855\n",
      "\n",
      "Precision/Recall/F-score: 0.6769709543568465 / 0.65 / 0.6632113821138211\n",
      "Epoch 49/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 1.0643\n",
      "\n",
      "Precision/Recall/F-score: 0.717242851045668 / 0.6695219123505977 / 0.6925613022872451\n",
      "Epoch 50/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9954\n",
      "\n",
      "Precision/Recall/F-score: 0.7297356346952152 / 0.6653386454183267 / 0.6960508492237157\n",
      "Epoch 51/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9311\n",
      "\n",
      "Precision/Recall/F-score: 0.7235180826021828 / 0.6735059760956176 / 0.6976168368926029\n",
      "Epoch 52/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8879\n",
      "\n",
      "Precision/Recall/F-score: 0.7321010762751521 / 0.6233067729083666 / 0.6733376371852808\n",
      "Epoch 53/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9666\n",
      "\n",
      "Precision/Recall/F-score: 0.7136465324384788 / 0.699003984063745 / 0.7062493710375364\n",
      "Epoch 54/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.9035\n",
      "\n",
      "Precision/Recall/F-score: 0.7279099405267629 / 0.6826693227091634 / 0.7045641447368421\n",
      "Epoch 55/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8657\n",
      "\n",
      "Precision/Recall/F-score: 0.731016731016731 / 0.6788844621513944 / 0.7039867795909935\n",
      "Epoch 56/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8438\n",
      "\n",
      "Precision/Recall/F-score: 0.7474032105760151 / 0.6306772908366534 / 0.6840968020743301\n",
      "Epoch 57/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8047\n",
      "\n",
      "Precision/Recall/F-score: 0.7157253831618727 / 0.6790836653386454 / 0.6969232341817437\n",
      "Epoch 58/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7917\n",
      "\n",
      "Precision/Recall/F-score: 0.7051800890327803 / 0.6942231075697212 / 0.6996587030716724\n",
      "Epoch 59/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7941\n",
      "\n",
      "Precision/Recall/F-score: 0.7141141141141141 / 0.7105577689243028 / 0.7123315027458811\n",
      "Epoch 60/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7696\n",
      "\n",
      "Precision/Recall/F-score: 0.6664824028008107 / 0.7205179282868526 / 0.6924475926103187\n",
      "Epoch 61/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7602\n",
      "\n",
      "Precision/Recall/F-score: 0.7335966644722405 / 0.6659362549800797 / 0.6981309387073196\n",
      "Epoch 62/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7560\n",
      "\n",
      "Precision/Recall/F-score: 0.7196670135275754 / 0.6888446215139442 / 0.7039185750636132\n",
      "Epoch 63/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7532\n",
      "\n",
      "Precision/Recall/F-score: 0.7497777777777778 / 0.6721115537848605 / 0.7088235294117646\n",
      "Epoch 64/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8194\n",
      "\n",
      "Precision/Recall/F-score: 0.7234404939322973 / 0.6768924302788845 / 0.6993928167129773\n",
      "Epoch 65/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7984\n",
      "\n",
      "Precision/Recall/F-score: 0.7323242794940908 / 0.7035856573705179 / 0.7176673778319619\n",
      "Epoch 66/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7866\n",
      "\n",
      "Precision/Recall/F-score: 0.7320943829609522 / 0.698406374501992 / 0.7148537057804057\n",
      "Epoch 67/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7446\n",
      "\n",
      "Precision/Recall/F-score: 0.7566739606126914 / 0.6888446215139442 / 0.7211678832116789\n",
      "Epoch 68/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7135\n",
      "\n",
      "Precision/Recall/F-score: 0.7521145087833442 / 0.6908366533864542 / 0.7201744367147752\n",
      "Epoch 69/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7200\n",
      "\n",
      "Precision/Recall/F-score: 0.7338235294117647 / 0.6958167330677291 / 0.7143149284253579\n",
      "Epoch 70/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7159\n",
      "\n",
      "Precision/Recall/F-score: 0.7076161790017211 / 0.6551792828685259 / 0.6803889118742241\n",
      "Epoch 71/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8517\n",
      "\n",
      "Precision/Recall/F-score: 0.7225793074849679 / 0.6942231075697212 / 0.7081174438687393\n",
      "Epoch 72/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.8186\n",
      "\n",
      "Precision/Recall/F-score: 0.6486439195100613 / 0.7384462151394422 / 0.6906380996739637\n",
      "Epoch 73/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7858\n",
      "\n",
      "Precision/Recall/F-score: 0.7481416703104504 / 0.6816733067729084 / 0.7133625182405671\n",
      "Epoch 74/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7592\n",
      "\n",
      "Precision/Recall/F-score: 0.7475090579710145 / 0.6575697211155378 / 0.6996608732513777\n",
      "Epoch 75/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 0.7825\n",
      "\n",
      "Precision/Recall/F-score: 0.705812101910828 / 0.7063745019920319 / 0.7060931899641577\n",
      "Epoch 76/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 4.5558\n",
      "\n",
      "Precision/Recall/F-score: 0.28527291452111225 / 0.11035856573705179 / 0.1591496696351623\n",
      "Epoch 77/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.7458\n",
      "\n",
      "Precision/Recall/F-score: 0.4312684365781711 / 0.14561752988047807 / 0.21772151898734174\n",
      "Epoch 78/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.3216\n",
      "\n",
      "Precision/Recall/F-score: 0.39287433798748195 / 0.16254980079681275 / 0.22995631957165\n",
      "Epoch 79/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.2043\n",
      "\n",
      "Precision/Recall/F-score: 0.2888130636119912 / 0.2360557768924303 / 0.25978296612956264\n",
      "Epoch 80/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.2420\n",
      "\n",
      "Precision/Recall/F-score: 0.3962836556693212 / 0.20816733067729085 / 0.2729528535980149\n",
      "Epoch 81/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.2424\n",
      "\n",
      "Precision/Recall/F-score: 0.384774728120145 / 0.14800796812749004 / 0.21378218961300532\n",
      "Epoch 82/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.2686\n",
      "\n",
      "Precision/Recall/F-score: 0.38913043478260867 / 0.17828685258964144 / 0.2445355191256831\n",
      "Epoch 83/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.2564\n",
      "\n",
      "Precision/Recall/F-score: 0.3656498119515253 / 0.17430278884462153 / 0.2360717658168083\n",
      "Epoch 84/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.1708\n",
      "\n",
      "Precision/Recall/F-score: 0.3553868402024584 / 0.1958167330677291 / 0.25250449524788077\n",
      "Epoch 85/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.1614\n",
      "\n",
      "Precision/Recall/F-score: 0.38317757009345793 / 0.19601593625498007 / 0.25935687928307855\n",
      "Epoch 86/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.1472\n",
      "\n",
      "Precision/Recall/F-score: 0.34019103600293904 / 0.18446215139442232 / 0.2392146732110566\n",
      "Epoch 87/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.1779\n",
      "\n",
      "Precision/Recall/F-score: 0.2762312633832976 / 0.25697211155378485 / 0.26625386996904027\n",
      "Epoch 88/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.1573\n",
      "\n",
      "Precision/Recall/F-score: 0.40146654445462876 / 0.1745019920318725 / 0.24326575951124688\n",
      "Epoch 89/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.1624\n",
      "\n",
      "Precision/Recall/F-score: 0.4188078108941418 / 0.16235059760956175 / 0.2339936836060867\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.1773\n",
      "\n",
      "Precision/Recall/F-score: 0.3263888888888889 / 0.24342629482071712 / 0.27886809675947055\n",
      "Epoch 91/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.2303\n",
      "\n",
      "Precision/Recall/F-score: 0.4186284544524053 / 0.16294820717131475 / 0.2345856036707772\n",
      "Epoch 92/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.2202\n",
      "\n",
      "Precision/Recall/F-score: 0.3221476510067114 / 0.24860557768924302 / 0.2806386327861479\n",
      "Epoch 93/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.3015\n",
      "\n",
      "Precision/Recall/F-score: 0.48444444444444446 / 0.13027888446215138 / 0.2053375196232339\n",
      "Epoch 94/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.4442\n",
      "\n",
      "Precision/Recall/F-score: 0.5187319884726225 / 0.10756972111553785 / 0.17818841775284608\n",
      "Epoch 95/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.5732\n",
      "\n",
      "Precision/Recall/F-score: 0.3835489833641405 / 0.16533864541832669 / 0.23106904231625836\n",
      "Epoch 96/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.5545\n",
      "\n",
      "Precision/Recall/F-score: 0.4106145251396648 / 0.11713147410358565 / 0.1822690638561686\n",
      "Epoch 97/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.5814\n",
      "\n",
      "Precision/Recall/F-score: 0.4254505734571273 / 0.1551792828685259 / 0.22741205663406805\n",
      "Epoch 98/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.5408\n",
      "\n",
      "Precision/Recall/F-score: 0.33764750666159116 / 0.17669322709163346 / 0.23198639989538378\n",
      "Epoch 99/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.4984\n",
      "\n",
      "Precision/Recall/F-score: 0.4529616724738676 / 0.10358565737051793 / 0.16861219195849547\n",
      "Epoch 100/100\n",
      "14041/14041 [==============================] - 27s 2ms/step - loss: 3.4919\n",
      "\n",
      "Precision/Recall/F-score: 0.3437902879243661 / 0.1593625498007968 / 0.21777596297808627\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "rnn = LSTM(100, activation='tanh', return_sequences=True)(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "lr_lstm=0.1\n",
    "batch_size_lstm=100\n",
    "epochs_lstm=100\n",
    "\n",
    "optimizer=Adam(lr=lr_lstm) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist_lstm=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=batch_size_lstm,verbose=1,epochs=epochs_lstm, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1, Batch size: 100, Epochs: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Learning rate: {}, Batch size: {}, Epochs: {}'.format(lr_lstm, batch_size_lstm, epochs_lstm))\n",
    "plt.plot(hist_lstm.history['loss'])\n",
    "#plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss, final: {}'.format(round(hist_lstm.history['loss'][-1], 3)))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 113, 200)          320800    \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 113, 4)            804       \n",
      "=================================================================\n",
      "Total params: 15,322,204\n",
      "Trainable params: 321,604\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "14041/14041 [==============================] - 53s 4ms/step - loss: 3.4025\n",
      "\n",
      "Precision/Recall/F-score: 0.6793907484016548 / 0.7197211155378486 / 0.6989746566066938\n",
      "Epoch 2/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 1.0455\n",
      "\n",
      "Precision/Recall/F-score: 0.6663712107782308 / 0.748804780876494 / 0.7051871306631649\n",
      "Epoch 3/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.8859\n",
      "\n",
      "Precision/Recall/F-score: 0.7512974051896207 / 0.749800796812749 / 0.7505483549351945\n",
      "Epoch 4/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.7663\n",
      "\n",
      "Precision/Recall/F-score: 0.7782990006118703 / 0.7601593625498008 / 0.7691222412576841\n",
      "Epoch 5/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.7025\n",
      "\n",
      "Precision/Recall/F-score: 0.8145363408521303 / 0.7768924302788844 / 0.7952691680261013\n",
      "Epoch 6/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.6375\n",
      "\n",
      "Precision/Recall/F-score: 0.8103049421661409 / 0.7675298804780877 / 0.7883375959079284\n",
      "Epoch 7/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5975\n",
      "\n",
      "Precision/Recall/F-score: 0.7775996794229614 / 0.7731075697211155 / 0.775347118170013\n",
      "Epoch 8/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5647\n",
      "\n",
      "Precision/Recall/F-score: 0.7724506730591923 / 0.8344621513944223 / 0.8022598870056498\n",
      "Epoch 9/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5386\n",
      "\n",
      "Precision/Recall/F-score: 0.7924966157416361 / 0.8163346613545817 / 0.8042390344421547\n",
      "Epoch 10/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5624\n",
      "\n",
      "Precision/Recall/F-score: 0.7766258246936852 / 0.8207171314741036 / 0.7980629539951574\n",
      "Epoch 11/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4951\n",
      "\n",
      "Precision/Recall/F-score: 0.8113641111343222 / 0.7737051792828685 / 0.792087284592638\n",
      "Epoch 12/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4967\n",
      "\n",
      "Precision/Recall/F-score: 0.794826224328594 / 0.8017928286852589 / 0.7982943276477588\n",
      "Epoch 13/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5242\n",
      "\n",
      "Precision/Recall/F-score: 0.8025568181818182 / 0.7878486055776892 / 0.7951347004423\n",
      "Epoch 14/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4774\n",
      "\n",
      "Precision/Recall/F-score: 0.7353479853479854 / 0.799800796812749 / 0.7662213740458015\n",
      "Epoch 15/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5135\n",
      "\n",
      "Precision/Recall/F-score: 0.7521647104098519 / 0.7786852589641434 / 0.7651952627972987\n",
      "Epoch 16/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5264\n",
      "\n",
      "Precision/Recall/F-score: 0.8186778593913956 / 0.7770916334661354 / 0.797342871742463\n",
      "Epoch 17/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4801\n",
      "\n",
      "Precision/Recall/F-score: 0.8140805179041068 / 0.801593625498008 / 0.807788818628927\n",
      "Epoch 18/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5016\n",
      "\n",
      "Precision/Recall/F-score: 0.7882376159463194 / 0.7956175298804781 / 0.7919103796966392\n",
      "Epoch 19/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4929\n",
      "\n",
      "Precision/Recall/F-score: 0.7968598565613491 / 0.8189243027888446 / 0.8077414284310838\n",
      "Epoch 20/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4891\n",
      "\n",
      "Precision/Recall/F-score: 0.8031797142282149 / 0.7950199203187251 / 0.7990789868855741\n",
      "Epoch 21/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4639\n",
      "\n",
      "Precision/Recall/F-score: 0.8268007387646213 / 0.8025896414342629 / 0.8145153138582837\n",
      "Epoch 22/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4455\n",
      "\n",
      "Precision/Recall/F-score: 0.7917072221140744 / 0.8101593625498008 / 0.8008270158511371\n",
      "Epoch 23/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4884\n",
      "\n",
      "Precision/Recall/F-score: 0.7917882856586884 / 0.8105577689243028 / 0.8010630967614921\n",
      "Epoch 24/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4908\n",
      "\n",
      "Precision/Recall/F-score: 0.8111788617886179 / 0.7950199203187251 / 0.8030181086519116\n",
      "Epoch 25/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4704\n",
      "\n",
      "Precision/Recall/F-score: 0.7903225806451613 / 0.8199203187250996 / 0.804849432929214\n",
      "Epoch 26/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4621\n",
      "\n",
      "Precision/Recall/F-score: 0.77902124430956 / 0.8181274900398406 / 0.7980956082394093\n",
      "Epoch 27/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4700\n",
      "\n",
      "Precision/Recall/F-score: 0.794668727062005 / 0.8195219123505976 / 0.8069039913700107\n",
      "Epoch 28/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4257\n",
      "\n",
      "Precision/Recall/F-score: 0.7377919320594479 / 0.8306772908366534 / 0.7814842578710643\n",
      "Epoch 29/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4379\n",
      "\n",
      "Precision/Recall/F-score: 0.8061953931691819 / 0.8087649402390438 / 0.807478122513922\n",
      "Epoch 30/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4334\n",
      "\n",
      "Precision/Recall/F-score: 0.8011695906432749 / 0.8187250996015937 / 0.8098522167487684\n",
      "Epoch 31/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4454\n",
      "\n",
      "Precision/Recall/F-score: 0.8126397073765494 / 0.796613545816733 / 0.8045468262750225\n",
      "Epoch 32/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4608\n",
      "\n",
      "Precision/Recall/F-score: 0.8077722360764755 / 0.7743027888446216 / 0.7906834825061025\n",
      "Epoch 33/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4738\n",
      "\n",
      "Precision/Recall/F-score: 0.8080264791063302 / 0.7780876494023904 / 0.7927745078140856\n",
      "Epoch 34/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5051\n",
      "\n",
      "Precision/Recall/F-score: 0.745451270041434 / 0.8243027888446215 / 0.782896603916375\n",
      "Epoch 35/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5325\n",
      "\n",
      "Precision/Recall/F-score: 0.8182382133995038 / 0.7882470119521913 / 0.8029626623376623\n",
      "Epoch 36/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.4920\n",
      "\n",
      "Precision/Recall/F-score: 0.7743824336688014 / 0.8430278884462151 / 0.8072484501669052\n",
      "Epoch 37/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5240\n",
      "\n",
      "Precision/Recall/F-score: 0.7736224388303163 / 0.7747011952191235 / 0.7741614412262366\n",
      "Epoch 38/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5842\n",
      "\n",
      "Precision/Recall/F-score: 0.7351764277919244 / 0.8051792828685259 / 0.7685871838752614\n",
      "Epoch 39/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5905\n",
      "\n",
      "Precision/Recall/F-score: 0.7993461381283203 / 0.7792828685258965 / 0.7891870082711316\n",
      "Epoch 40/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.7297\n",
      "\n",
      "Precision/Recall/F-score: 0.7147842056932966 / 0.7752988047808765 / 0.7438127090301003\n",
      "Epoch 41/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.7361\n",
      "\n",
      "Precision/Recall/F-score: 0.8158013544018059 / 0.7199203187250997 / 0.7648677248677249\n",
      "Epoch 42/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5874\n",
      "\n",
      "Precision/Recall/F-score: 0.7935640558591378 / 0.7810756972111553 / 0.7872703543820901\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5423\n",
      "\n",
      "Precision/Recall/F-score: 0.7516495601173021 / 0.8169322709163347 / 0.7829324169530355\n",
      "Epoch 44/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5145\n",
      "\n",
      "Precision/Recall/F-score: 0.801679983609916 / 0.7794820717131474 / 0.7904252095747903\n",
      "Epoch 45/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5343\n",
      "\n",
      "Precision/Recall/F-score: 0.7735482617984074 / 0.7934262948207171 / 0.7833611957911298\n",
      "Epoch 46/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.5108\n",
      "\n",
      "Precision/Recall/F-score: 0.7973568281938326 / 0.7932270916334662 / 0.7952865987617335\n",
      "Epoch 47/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.6098\n",
      "\n",
      "Precision/Recall/F-score: 0.7657077971233913 / 0.8059760956175299 / 0.7853260869565216\n",
      "Epoch 48/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.6448\n",
      "\n",
      "Precision/Recall/F-score: 0.7329629629629629 / 0.7884462151394422 / 0.7596928982725528\n",
      "Epoch 49/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.7192\n",
      "\n",
      "Precision/Recall/F-score: 0.7830762987012987 / 0.7687250996015936 / 0.7758343385605146\n",
      "Epoch 50/50\n",
      "14041/14041 [==============================] - 52s 4ms/step - loss: 0.6416\n",
      "\n",
      "Precision/Recall/F-score: 0.7958937198067633 / 0.7876494023904382 / 0.7917501001201442\n"
     ]
    }
   ],
   "source": [
    "inp=Input(shape=(sequence_len,))\n",
    "embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "rnn = Bidirectional(LSTM(100, activation='tanh', return_sequences=True))(embeddings)\n",
    "outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "model=Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "lr_bilstm=0.1\n",
    "batch_size_bilstm=100\n",
    "epochs_bilstm=50\n",
    "\n",
    "optimizer=Adam(lr=lr_bilstm) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# train\n",
    "hist_bilstm=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=batch_size_bilstm,verbose=1,epochs=epochs_bilstm, callbacks=[EvaluateEntities()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_bilstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d285dff1cb6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Learning rate: {}, Batch size: {}, Epochs: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_bilstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_bilstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_bilstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_bilstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.plot(hist.history['val_loss'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss, final: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_bilstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_bilstm' is not defined"
     ]
    }
   ],
   "source": [
    "print('Learning rate: {}, Batch size: {}, Epochs: {}'.format(lr_bilstm, batch_size_bilstm, epochs_bilstm))\n",
    "plt.plot(hist_bilstm.history['loss'])\n",
    "#plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss, final: {}'.format(round(hist_bilstm.history['loss'][-1], 3)))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence> Thomas\n",
      "Thomas I-PER\n",
      "sentence> London\n",
      "London I-LOC\n",
      "sentence> Finland\n",
      "Finland I-LOC\n",
      "sentence> Microsoft\n",
      "Microsoft I-ORG\n",
      "sentence> Turku\n",
      "Turku I-ORG\n",
      "sentence> Nokia\n",
      "Nokia I-ORG\n",
      "sentence> Apple\n",
      "Apple I-ORG\n",
      "sentence> Google\n",
      "Google O\n",
      "sentence> Hello hows it going\n",
      "Hello O\n",
      "hows I-PER\n",
      "it O\n",
      "going O\n",
      "sentence> Democrats were increasingly optimistic that they had pushed their favored candidates into the general election in several important races in Southern California\n",
      "Democrats O\n",
      "were O\n",
      "increasingly O\n",
      "optimistic O\n",
      "that O\n",
      "they O\n",
      "had O\n",
      "pushed O\n",
      "their O\n",
      "favored O\n",
      "candidates O\n",
      "into O\n",
      "the O\n",
      "general O\n",
      "election O\n",
      "in O\n",
      "several O\n",
      "important O\n",
      "races O\n",
      "in O\n",
      "Southern I-LOC\n",
      "California I-LOC\n",
      "sentence> Southern\n",
      "Southern I-ORG\n",
      "sentence> Gavin Newsom, a Democratic former mayor of San Francisco, and John Cox, a Republican backed by President Trump, won the two spots in the race to succeed Gov. Jerry Brown\n",
      "Gavin I-PER\n",
      "Newsom, I-PER\n",
      "a O\n",
      "Democratic O\n",
      "former O\n",
      "mayor O\n",
      "of O\n",
      "San I-LOC\n",
      "Francisco, I-LOC\n",
      "and O\n",
      "John I-PER\n",
      "Cox, I-PER\n",
      "a O\n",
      "Republican O\n",
      "backed O\n",
      "by O\n",
      "President O\n",
      "Trump, I-PER\n",
      "won O\n",
      "the O\n",
      "two O\n",
      "spots O\n",
      "in O\n",
      "the O\n",
      "race O\n",
      "to O\n",
      "succeed O\n",
      "Gov. O\n",
      "Jerry I-PER\n",
      "Brown I-PER\n",
      "sentence> Races in New Jersey, Iowa and other states were also crucial. Mikie Sherrill, a former Navy pilot and federal prosecutor, won the Democratic nomination in the 11th District in New Jersey\n",
      "Races O\n",
      "in O\n",
      "New I-LOC\n",
      "Jersey, O\n",
      "Iowa I-LOC\n",
      "and O\n",
      "other O\n",
      "states O\n",
      "were O\n",
      "also O\n",
      "crucial. O\n",
      "Mikie O\n",
      "Sherrill, O\n",
      "a O\n",
      "former O\n",
      "Navy I-ORG\n",
      "pilot O\n",
      "and O\n",
      "federal O\n",
      "prosecutor, O\n",
      "won O\n",
      "the O\n",
      "Democratic O\n",
      "nomination O\n",
      "in O\n",
      "the O\n",
      "11th O\n",
      "District O\n",
      "in O\n",
      "New I-LOC\n",
      "Jersey I-LOC\n",
      "sentence> New Jersey\n",
      "New I-LOC\n",
      "Jersey I-LOC\n",
      "sentence> New Jersey,\n",
      "New O\n",
      "Jersey, O\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d2df4c11846a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inverse_label_map = {value: key for key, value in label_map.items()}\n",
    "\n",
    "def tag_sentence(sentence):\n",
    "    tokens = sentence.split() # Stupid whitespace tokenization\n",
    "    vectorized_sentence, _, sentence_length=vectorizer(vocabulary, [tokens], label_map) # Using our global variables again...\n",
    "    vectorized_sentence_padded = pad_sequences(vectorized_sentence, padding='post', maxlen=max(lengths)) # Pad the sequence\n",
    "\n",
    "    predictions = model.predict(vectorized_sentence_padded)[0] # Everything so far has been a 'list' of sentences with a single sentence, so we only take index 0\n",
    "    predictions = numpy.argmax(predictions, axis=-1) # Take the tag index with the highest value for each token\n",
    "    \n",
    "    tags = [inverse_label_map[label_index] for label_index in predictions[:len(tokens)]] # Ignore padded region\n",
    "    return tags, tokens\n",
    "    \n",
    "while True:\n",
    "    sentence=input(\"sentence> \")\n",
    "    if sentence==\"end\":\n",
    "        break\n",
    "    tags, tokens = tag_sentence(sentence)\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        print(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
